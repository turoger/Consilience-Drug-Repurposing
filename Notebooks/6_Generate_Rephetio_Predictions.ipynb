{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using the Rephetio algorithm\n",
    "* This notebook follows the approach of the [MechRepoNet paper](https://github.com/SuLab/MechRepoNet/tree/d219c7e2540513c2338d6623348697142a08f73a)\n",
    "* Please generate the `nodes_biolink.csv` and `edges_biolink.csv` from the original code\n",
    "\n",
    "* This notebook executes the following steps:\n",
    "    * Generate a holdout set from the 'test' and 'valid' splits\n",
    "    * do some preprocessing of the data\n",
    "    * Find best hyperparameters\n",
    "    * Make predictions\n",
    "    \n",
    "* running the entire notebook can take up to a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogertu/miniforge3/envs/kge/lib/python3.11/site-packages/data_tools/df_processing.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# dataset generation\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# prerequisites\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# hpo\n",
    "from pathlib import Path\n",
    "import re as regex\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import re as regex\n",
    "from itertools import chain\n",
    "import data_tools.graphs as gt\n",
    "from hetnet_ml.extractor import MatrixFormattedGraph, piecewise_extraction\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from data_tools.plotting import count_plot_h\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the relative directory get to the root\n",
    "os.chdir(\"./Consilience-Drug-Repurposing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a holdout set for MIND from original MRN dataset\n",
    "* MIND is in a triple format\n",
    "* MRN is in a Neo4j format (property tables)\n",
    "* In this section, we shoe horn MIND into the rephetio algorithm by mapping MIND edges to MRN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import in files\n",
    "* nodes/edges\n",
    "* MIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MIND\n",
    "mind_edges = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./data/MIND/train.txt\", sep=\"\\t\", names=[\"h\", \"r\", \"t\"]).assign(\n",
    "            label=\"train\"\n",
    "        ),\n",
    "        pd.read_csv(\"./data/MIND/valid.txt\", sep=\"\\t\", names=[\"h\", \"r\", \"t\"]).assign(\n",
    "            label=\"valid\"\n",
    "        ),\n",
    "        pd.read_csv(\"./data/MIND/test.txt\", sep=\"\\t\", names=[\"h\", \"r\", \"t\"]).assign(\n",
    "            label=\"test\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# import biolink data from the data folder. This is from MechRepoNet\n",
    "nodes = pd.read_csv(\"./data/nodes_biolink.csv\", dtype=str)\n",
    "edges = pd.read_csv(\"./data/edges_biolink.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate nodes and edges file\n",
    "* filter imported nodes file from the concatenation of the head and tail of the MRN edges \n",
    "* get appropriate edge types for every edge in MRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the nodes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 249,605\n"
     ]
    }
   ],
   "source": [
    "mind_nodes = list(set(mind_edges[\"h\"]).union(set(mind_edges[\"t\"])))\n",
    "\n",
    "# all nodes in mind are nested in the nodes file.\n",
    "mind_nodes = nodes.query(\"id in @mind_nodes\")[[\"id\", \"name\", \"label\"]]\n",
    "\n",
    "print(f\"Number of nodes: {len(mind_nodes):,}\")\n",
    "print(f\"All nodes in MIND: {mind_nodes.shape[0]==249605}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the edges file\n",
    "* export both nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNII:BTY153760O</td>\n",
       "      <td>inhibits_CinG</td>\n",
       "      <td>NCBIGene:3605</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEBI:10056</td>\n",
       "      <td>activates_CaG</td>\n",
       "      <td>NCBIGene:1129</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:10056</td>\n",
       "      <td>activates_CaG</td>\n",
       "      <td>NCBIGene:1131</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:10056</td>\n",
       "      <td>activates_CaG</td>\n",
       "      <td>NCBIGene:1133</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:10056</td>\n",
       "      <td>activates_CaG</td>\n",
       "      <td>NCBIGene:3350</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 h              r              t  label\n",
       "0  UNII:BTY153760O  inhibits_CinG  NCBIGene:3605  train\n",
       "1      CHEBI:10056  activates_CaG  NCBIGene:1129  train\n",
       "2      CHEBI:10056  activates_CaG  NCBIGene:1131  train\n",
       "3      CHEBI:10056  activates_CaG  NCBIGene:1133  train\n",
       "4      CHEBI:10056  activates_CaG  NCBIGene:3350  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mind_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add metapath info to indication edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev = {\n",
    "    \"AnatomicalEntity\": \"A\",\n",
    "    \"BiologicalProcessOrActivity\": \"BP\",\n",
    "    \"MacromolecularMachine\": \"G\",\n",
    "    \"ChemicalSubstance\": \"C\",\n",
    "    \"Disease\": \"D\",\n",
    "    \"PhenotypicFeature\": \"P\",\n",
    "    \"Pathway\": \"PW\",\n",
    "    \"GeneFamily\": \"F\",\n",
    "    \"OrganismTaxon\": \"T\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by indication and non-indication\n",
    "mind_edges_not_ind = mind_edges[~mind_edges[\"r\"].str.contains(\"indication\")]\n",
    "mind_edges_ind = mind_edges[mind_edges[\"r\"].str.contains(\"indication\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>label</th>\n",
       "      <th>h_lab</th>\n",
       "      <th>t_lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:32120</td>\n",
       "      <td>indication</td>\n",
       "      <td>REACT:R-HSA-2160456</td>\n",
       "      <td>train</td>\n",
       "      <td>ChemicalSubstance</td>\n",
       "      <td>Pathway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UBERON:0001132</td>\n",
       "      <td>indication</td>\n",
       "      <td>DOID:11199</td>\n",
       "      <td>train</td>\n",
       "      <td>AnatomicalEntity</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:10023</td>\n",
       "      <td>indication</td>\n",
       "      <td>DOID:0050289</td>\n",
       "      <td>train</td>\n",
       "      <td>ChemicalSubstance</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:100241</td>\n",
       "      <td>indication</td>\n",
       "      <td>DOID:12385</td>\n",
       "      <td>train</td>\n",
       "      <td>ChemicalSubstance</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:100241</td>\n",
       "      <td>indication</td>\n",
       "      <td>DOID:13258</td>\n",
       "      <td>train</td>\n",
       "      <td>ChemicalSubstance</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                h           r                    t  label              h_lab  \\\n",
       "0     CHEBI:32120  indication  REACT:R-HSA-2160456  train  ChemicalSubstance   \n",
       "1  UBERON:0001132  indication           DOID:11199  train   AnatomicalEntity   \n",
       "2     CHEBI:10023  indication         DOID:0050289  train  ChemicalSubstance   \n",
       "3    CHEBI:100241  indication           DOID:12385  train  ChemicalSubstance   \n",
       "4    CHEBI:100241  indication           DOID:13258  train  ChemicalSubstance   \n",
       "\n",
       "     t_lab  \n",
       "0  Pathway  \n",
       "1  Disease  \n",
       "2  Disease  \n",
       "3  Disease  \n",
       "4  Disease  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double merge your nodes to get the labels\n",
    "mind_edges_ind = (\n",
    "    # get h_label\n",
    "    mind_edges_ind.merge(mind_nodes, left_on=\"h\", right_on=\"id\", how=\"left\")[\n",
    "        [\"h\", \"r\", \"t\", \"label_x\", \"label_y\"]\n",
    "    ]\n",
    "    # get t_label\n",
    "    .merge(mind_nodes, left_on=\"t\", right_on=\"id\", how=\"left\").rename(\n",
    "        columns={\n",
    "            \"label\": \"t_lab\",\n",
    "            \"label_x\": \"label\",\n",
    "            \"label_y\": \"h_lab\",\n",
    "        }\n",
    "    )[[\"h\", \"r\", \"t\", \"label\", \"h_lab\", \"t_lab\"]]\n",
    ")\n",
    "mind_edges_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate using the abbrevation dictionary\n",
    "mind_edges_ind[\"h_lab\"] = mind_edges_ind[\"h_lab\"].map(lambda x: abbrev[x])\n",
    "mind_edges_ind[\"t_lab\"] = mind_edges_ind[\"t_lab\"].map(lambda x: abbrev[x])\n",
    "\n",
    "# remake r\n",
    "mind_edges_ind[\"r\"] = (\n",
    "    \"indication\" + \"_\" + mind_edges_ind[\"h_lab\"] + \"i\" + mind_edges_ind[\"t_lab\"]\n",
    ")\n",
    "\n",
    "mind_edges_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>CHEBI:9667</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>CHEBI:41423</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>CHEBI:9168</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:7490</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>CHEBI:41879</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>CHEBI:8378</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                h               r              t label\n",
       "5365   CHEBI:9667  indication_CiG   NCBIGene:367  test\n",
       "5366  CHEBI:41423  indication_CiG   NCBIGene:367  test\n",
       "5367   CHEBI:9168  indication_CiG  NCBIGene:7490  test\n",
       "5368  CHEBI:41879  indication_CiG   NCBIGene:367  test\n",
       "5369   CHEBI:8378  indication_CiG   NCBIGene:367  test"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restack indications and non-indications\n",
    "mind_edges = pd.concat([mind_edges_not_ind, mind_edges_ind[[\"h\", \"r\", \"t\", \"label\"]]])\n",
    "mind_edges.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_nodes.to_csv(\"./data/nodes_rep.csv\", index=False)\n",
    "mind_edges.rename(columns={\"h\": \"start_id\", \"r\": \"type\", \"t\": \"end_id\"}, inplace=True)\n",
    "mind_edges[[\"start_id\", \"type\", \"end_id\"]].to_csv(\"./data/edges_rep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout is test\n",
    "remain_edges_no_test = mind_edges.query('label!=\"test\"')[[\"start_id\", \"type\", \"end_id\"]]\n",
    "remain_edges_no_test.to_csv(\"./data/remain_edges_no-test.csv\", index=False)\n",
    "\n",
    "# export test holdout\n",
    "holdout_edges_test = mind_edges.query('label==\"test\"')[[\"start_id\", \"type\", \"end_id\"]]\n",
    "holdout_edges_test.to_csv(\"./data/holdout_edges_test.csv\", index=False)\n",
    "\n",
    "# holdout is valid\n",
    "remain_edges_no_valid = mind_edges.query('label!=\"valid\"')[\n",
    "    [\"start_id\", \"type\", \"end_id\"]\n",
    "]\n",
    "remain_edges_no_valid.to_csv(\"./data/remain_edges_no-valid.csv\", index=False)\n",
    "\n",
    "# export valid holdout\n",
    "holdout_edges_valid = mind_edges.query('label==\"valid\"')[[\"start_id\", \"type\", \"end_id\"]]\n",
    "holdout_edges_valid.to_csv(\"./data/holdout_edges_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metapath files (prerequisites for HPO and testing)\n",
    "* To run rephetio, metapath counts must first be calculated\n",
    "* We calculate the metapath counts for:\n",
    "    * all metapaths\n",
    "    * all positive metapaths (known trues)\n",
    "    * all idealized training metapaths (metapaths from DrugMechDB v1)\n",
    "    * all negative metapaths (non-trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get edge info and add indications to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edge information\n",
    "e_info = pd.read_csv(\n",
    "    \"/home/rogertu/projects/Consilience-Drug-Repurposing/path-based/rephetio/0_data/manual/edge_semtypes.csv\"\n",
    ")\n",
    "\n",
    "# add indication edge information\n",
    "add_indication = pd.DataFrame(\n",
    "    {\n",
    "        \"fwd_edge\": [\"indication\"],\n",
    "        \"abbrev\": [\"i\"],\n",
    "        \"rev_edge\": [\"indication_of\"],\n",
    "        \"rel_dir\": [-1],\n",
    "        \"directed\": [True],\n",
    "        \"parent_rel\": [np.nan],\n",
    "    }\n",
    ")\n",
    "\n",
    "# create new edge information\n",
    "e_info = pd.concat([e_info, add_indication])\n",
    "directed_map = e_info.set_index(\"fwd_edge\")[\"directed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind_nodes = pd.read_csv(\"./data/nodes_rep.csv\")\n",
    "remain_edges_no_test = pd.read_csv(\"./data/remain_edges_no-test.csv\")\n",
    "remain_edges_no_valid = pd.read_csv(\"./data/remain_edges_no-valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the metapaths\n",
    "\n",
    "# edges are the remaining edges, no test or valid\n",
    "nodes_ls, edges_ls, mps_ls = list(), list(), list()\n",
    "\n",
    "# add nodes and edges\n",
    "nodes_ls.append(mind_nodes)\n",
    "nodes_ls.append(mind_nodes)\n",
    "edges_ls.append(remain_edges_no_test)\n",
    "edges_ls.append(remain_edges_no_valid)\n",
    "\n",
    "# create the metapath files\n",
    "for i, v in enumerate(edges_ls):\n",
    "    mps = gt.dataframes_to_metagraph(nodes_ls[i], v).extract_metapaths(\n",
    "        \"ChemicalSubstance\", \"Disease\", 4\n",
    "    )\n",
    "    mps = [mp for mp in mps if len(mp) > 1]\n",
    "    mps_ls.append(mps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at similarity and non-similarity based paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                     | 0/30567 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 30567/30567 [00:01<00:00, 23026.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 26484/26484 [00:01<00:00, 22939.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# look for similarity based paths\n",
    "similarity_paths_check_ls = list()\n",
    "for mps in mps_ls:\n",
    "    similarity_paths_check = []\n",
    "\n",
    "    for mp in tqdm(mps):\n",
    "        if len(mp) < 2:\n",
    "            similarity_paths_check.append(True)\n",
    "            continue\n",
    "        this_path = []\n",
    "        for label in [\"ChemicalSubstance\"]:\n",
    "            this_path.append(gt.is_similarity(mp, [\"ChemicalSubstance\"], directed_map))\n",
    "        this_path.append(gt.is_similarity(mp, \"Disease\", directed_map, max_repeats=1))\n",
    "\n",
    "        # Don't want CtXaYzD edges.. Ct (and Cm) edges should be banned...\n",
    "        # Though not strictly similarity edges, they have heavy implications for Disease Phenotype similarity\n",
    "        bl_edge = [\"Ct\", \"Cm\", \"Cdg\", \"Cpl\", \"Cpv\", \"Ci\"]\n",
    "        for bl in bl_edge:\n",
    "            if mp.abbrev.startswith(bl) or mp.abbrev.endswith(\"aw\" + bl + \"D\"):\n",
    "                this_path.append(True)\n",
    "\n",
    "        similarity_paths_check.append(sum(this_path) > 0)\n",
    "\n",
    "    similarity_paths_check_ls.append(similarity_paths_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique metapaths0: 8,198\n",
      "Unique metapaths1: 8,198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get non-similarity paths\n",
    "non_sim_mps_ls = [\n",
    "    [mp for mp, sim in zip(mps, similarity_paths_check) if not sim] for mps in mps_ls\n",
    "]\n",
    "[\n",
    "    print(f\"Unique metapaths{i}: {len(non_sim_mps):,}\")\n",
    "    for i, non_sim_mps in enumerate(non_sim_mps_ls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.82% of all metapaths are not similarity based\n",
      "30.95% of all metapaths are not similarity based\n"
     ]
    }
   ],
   "source": [
    "for i, non_sim_mps in enumerate(non_sim_mps_ls):\n",
    "    print(\n",
    "        f\"{len(non_sim_mps)/len(mps_ls[i]):.2%} of all metapaths are not similarity based\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate metagraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating graph ...\n",
      "Processing node and edge data...\n",
      "Initializing metagraph...\n",
      "Generating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:52<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining degrees for each node and metaedge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:25<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighting matrices by degree with dampening factor 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 80.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing node and edge data...\n",
      "Initializing metagraph...\n",
      "Generating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:54<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining degrees for each node and metaedge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:26<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighting matrices by degree with dampening factor 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 84.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating graph ...\")\n",
    "mg_ls = [\n",
    "    MatrixFormattedGraph(\n",
    "        nodes_ls[i],\n",
    "        edges_ls[i],\n",
    "        \"ChemicalSubstance\",\n",
    "        \"Disease\",\n",
    "        max_length=4,\n",
    "        w=0.4,\n",
    "        n_jobs=30,\n",
    "    )\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the memory required to extract all compound-disease pairs\n",
    "float_size = 32  # bits\n",
    "bits_per_gb = 8589934592\n",
    "\n",
    "\n",
    "def print_mem_info(n_comp, n_dis, n_mps):\n",
    "    print(f\"{n_comp:,} Compounds * {n_dis:,} Diseases = {n_comp * n_dis:,} C-D Pairs\")\n",
    "    print(\n",
    "        f\"{n_comp * n_dis:,} C-D Pairs * {n_mps:,} Metapaths = {n_comp * n_dis * n_mps:,} Matrix Values\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{n_comp * n_dis * n_mps * float_size / (bits_per_gb):1,.1f} GB of matrix values\"\n",
    "    )\n",
    "    print(f\"{n_comp * n_dis * float_size / (bits_per_gb):1,.3f} GB per metapath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40,455 Compounds * 13,942 Diseases = 564,023,610 C-D Pairs\n",
      "564,023,610 C-D Pairs * 8,198 Metapaths = 4,623,865,554,780 Matrix Values\n",
      "17,225.2 GB of matrix values\n",
      "2.101 GB per metapath\n",
      "\n",
      "40,455 Compounds * 13,942 Diseases = 564,023,610 C-D Pairs\n",
      "564,023,610 C-D Pairs * 8,198 Metapaths = 4,623,865,554,780 Matrix Values\n",
      "17,225.2 GB of matrix values\n",
      "2.101 GB per metapath\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at total compounds and diseases and estimate metapath memory usage\n",
    "total_comps_ls, total_dis_ls = list(), list()\n",
    "for i, nodes in enumerate(nodes_ls):\n",
    "    total_comps = nodes[\"label\"].value_counts()[\"ChemicalSubstance\"]\n",
    "    total_dis = nodes[\"label\"].value_counts()[\"Disease\"]\n",
    "\n",
    "    total_comps_ls.append(total_comps)\n",
    "    total_dis_ls.append(total_dis)\n",
    "    print_mem_info(total_comps, total_dis, len(non_sim_mps_ls[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_extraction(function, to_split, block_size=1000, **params):\n",
    "\n",
    "    assert type(to_split) == str and to_split in params\n",
    "\n",
    "    # Won't want progress bars for each subsetx\n",
    "    params[\"verbose\"] = False\n",
    "\n",
    "    # Retain a copy of the original parameters\n",
    "    full_params = deepcopy(params)\n",
    "    total = len(params[to_split])\n",
    "\n",
    "    # Determine the number of iterations needed\n",
    "    num_iter = total // block_size\n",
    "    if total % block_size != 0:\n",
    "        num_iter += 1\n",
    "\n",
    "    all_results = []\n",
    "    for i in tqdm(range(num_iter)):\n",
    "\n",
    "        # Get the start and end indicies\n",
    "        start = i * block_size\n",
    "        end = (i + 1) * block_size\n",
    "\n",
    "        # End can't be larger than the total number items\n",
    "        if end > total:\n",
    "            end = total\n",
    "\n",
    "        # Subset the paramter of interest\n",
    "        params[to_split] = full_params[to_split][start:end]\n",
    "\n",
    "        # Get the funciton results\n",
    "        all_results.append(function(**params))\n",
    "\n",
    "    return pd.concat(all_results, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metapath pair counts for each metapath in both test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metapath pair counts ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# This can take like 4 hours\n",
    "mp_counts_ls = list()\n",
    "\n",
    "print(f\"Extracting metapath pair counts ...\")\n",
    "\n",
    "for i, mps in enumerate(mps_ls):\n",
    "    file_loc = os.path.join(\"./path-based/rephetio/0_data\", f\"mp_counts0_{i}.pkl\")\n",
    "\n",
    "    if os.path.exists(file_loc):\n",
    "        with open(file_loc, \"rb\") as f:\n",
    "            mp_counts = pickle.load(f)\n",
    "\n",
    "        mp_counts_ls.append(mp_counts)\n",
    "\n",
    "    else:\n",
    "        mp_counts = piecewise_extraction(\n",
    "            function=mg_ls[i].extract_metapath_pair_counts,\n",
    "            to_split=\"metapaths\",\n",
    "            block_size=200,\n",
    "            metapaths=[mp.abbrev for mp in mps],\n",
    "            start_nodes=\"ChemicalSubstance\",\n",
    "            end_nodes=\"Disease\",\n",
    "            n_jobs=30,\n",
    "        )\n",
    "        mp_counts[\"subset\"] = \"all_pairs\"\n",
    "        mp_counts[\"frac\"] = (\n",
    "            mp_counts[\"pair_count\"] / total_comps_ls[i] * total_dis_ls[i]\n",
    "        )\n",
    "\n",
    "        mp_counts_ls.append(mp_counts)\n",
    "        with open(file_loc, \"wb\") as f:\n",
    "            pickle.dump(mp_counts, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of paths populated for 0 object: 70.18%\n",
      "Percent of paths populated for 1 object: 75.45%\n"
     ]
    }
   ],
   "source": [
    "# get paths populated for each metapath object\n",
    "for i, mp_counts in enumerate(mp_counts_ls):\n",
    "    print(\n",
    "        f'Percent of paths populated for {i} object: {(mp_counts[\"pair_count\"] != 0).sum() / len(mp_counts):1.2%}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get positive metapaths and training positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All potential training positives\n",
      "11,352 Compounds * 5,631 Diseases = 63,923,112 C-D Pairs\n",
      "63,923,112 C-D Pairs * 8,198 Metapaths = 524,041,672,176 Matrix Values\n",
      "1,952.2 GB of matrix values\n",
      "0.238 GB per metapath\n",
      "11,350 Compounds * 5,627 Diseases = 63,866,450 C-D Pairs\n",
      "63,866,450 C-D Pairs * 8,198 Metapaths = 523,577,157,100 Matrix Values\n",
      "1,950.5 GB of matrix values\n",
      "0.238 GB per metapath\n"
     ]
    }
   ],
   "source": [
    "# All potential training Positives\n",
    "keep_comps_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\" | type == \"indication_CiD\"')[\"start_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "\n",
    "keep_dis_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\"| type==\"indication_CiD\"')[\"end_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "\n",
    "print(\"All potential training positives\")\n",
    "for i, keep_comps in enumerate(keep_comps_ls):\n",
    "    print_mem_info(len(keep_comps), len(keep_dis_ls[i]), len(non_sim_mps_ls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting positive metapath pair counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [58:56<00:00, 57.04s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [48:20<00:00, 54.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This can take about 5 hours\n",
    "print(f\"Extracting positive metapath pair counts ...\")\n",
    "\n",
    "mp_counts_ls2 = list()\n",
    "for i, mg in enumerate(mg_ls):\n",
    "    file_loc = os.path.join(\"./path-based/rephetio/0_data\", f\"mp_counts1_{i}.pkl\")\n",
    "\n",
    "    if os.path.exists(file_loc):\n",
    "        with open(file_loc, \"rb\") as f:\n",
    "            mp_counts = pickle.load(f)\n",
    "\n",
    "        mp_counts_ls.append(mp_counts)\n",
    "    else:\n",
    "\n",
    "        mp_counts = piecewise_extraction(\n",
    "            function=mg.extract_metapath_pair_counts,\n",
    "            to_split=\"metapaths\",\n",
    "            block_size=500,\n",
    "            metapaths=[mp.abbrev for mp in mps_ls[i]],\n",
    "            start_nodes=list(keep_comps_ls[i]),\n",
    "            end_nodes=list(keep_dis_ls[i]),\n",
    "            n_jobs=30,\n",
    "        )\n",
    "        mp_counts[\"subset\"] = \"all_pos\"\n",
    "        mp_counts[\"frac\"] = mp_counts[\"pair_count\"] / (\n",
    "            len(keep_comps_ls[i]) * len(keep_dis_ls[i])\n",
    "        )\n",
    "\n",
    "        mp_counts_ls2.append(mp_counts)\n",
    "        with open(file_loc, \"wb\") as f:\n",
    "            pickle.dump(mp_counts, f)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of paths populated: 69.05%\n",
      "Percent of paths populated: 74.30%\n"
     ]
    }
   ],
   "source": [
    "for i, mp_counts in enumerate(mp_counts_ls2):\n",
    "    print(\n",
    "        f'Percent of paths populated: {(mp_counts[\"pair_count\"] != 0).sum() / len(mp_counts):1.2%}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get idealized training subset metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_group = [\n",
    "    \"indication_CiD\",\n",
    "    \"indication_CiG\",\n",
    "    \"indication_CiP\",\n",
    "    \"indication_CiPW\",\n",
    "    \"treats_CtP\",\n",
    "]\n",
    "\n",
    "dis_group = [\n",
    "    \"treats_GtD\",\n",
    "    \"indication_AiD\",\n",
    "    \"indication_CiD\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14,543 Compounds * 6,510 Diseases = 94,674,930 C-D Pairs\n",
      "94,674,930 C-D Pairs * 8,198 Metapaths = 776,145,076,140 Matrix Values\n",
      "2,891.4 GB of matrix values\n",
      "0.353 GB per metapath\n",
      "14,540 Compounds * 6,506 Diseases = 94,597,240 C-D Pairs\n",
      "94,597,240 C-D Pairs * 8,198 Metapaths = 775,508,173,520 Matrix Values\n",
      "2,889.0 GB of matrix values\n",
      "0.352 GB per metapath\n"
     ]
    }
   ],
   "source": [
    "# Ideal Training Subset\n",
    "\n",
    "neg_frac = 0.1\n",
    "rs = 1234\n",
    "\n",
    "keep_comps_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\" | type in @comp_group')[\"start_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "keep_comps_ls = [\n",
    "    keep_comps\n",
    "    | set(\n",
    "        nodes.query('id not in @keep_comps and label == \"ChemicalSubstance\"').sample(\n",
    "            frac=neg_frac, random_state=rs\n",
    "        )[\"id\"]\n",
    "    )\n",
    "    for keep_comps in keep_comps_ls\n",
    "]\n",
    "\n",
    "keep_dis_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\" | type in @dis_group')[\"end_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "keep_dis_ls = [\n",
    "    keep_dis\n",
    "    | set(\n",
    "        nodes.query('label == \"Disease\" and id not in @keep_dis').sample(\n",
    "            frac=neg_frac, random_state=rs + 1\n",
    "        )[\"id\"]\n",
    "    )\n",
    "    for keep_dis in keep_dis_ls\n",
    "]\n",
    "\n",
    "for i, keep_comps in enumerate(keep_comps_ls):\n",
    "    print_mem_info(len(keep_comps), len(keep_dis_ls[i]), len(non_sim_mps_ls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [56:43<00:00, 54.90s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [50:52<00:00, 57.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This can take like 2 hours\n",
    "print(\"Extracting training samples ...\")\n",
    "\n",
    "mp_counts_ls3 = list()\n",
    "for i, mg in enumerate(mg_ls):\n",
    "    file_loc = os.path.join(\"./path-based/rephetio/0_data\", f\"mp_counts2_{i}.pkl\")\n",
    "\n",
    "    if os.path.exists(file_loc):\n",
    "        with open(file_loc, \"rb\") as f:\n",
    "            mp_counts = pickle.load(f)\n",
    "\n",
    "        mp_counts_ls.append(mp_counts)\n",
    "    else:\n",
    "        mp_counts = piecewise_extraction(\n",
    "            function=mg.extract_metapath_pair_counts,\n",
    "            to_split=\"metapaths\",\n",
    "            block_size=500,\n",
    "            metapaths=[mp.abbrev for mp in mps_ls[i]],\n",
    "            start_nodes=list(keep_comps_ls[i]),\n",
    "            end_nodes=list(keep_dis_ls[i]),\n",
    "            n_jobs=30,\n",
    "        )\n",
    "        mp_counts[\"subset\"] = \"traning_sample\"\n",
    "        mp_counts[\"frac\"] = mp_counts[\"pair_count\"] / (\n",
    "            len(keep_comps_ls[i]) * len(keep_dis_ls[i])\n",
    "        )\n",
    "\n",
    "        mp_counts_ls3.append(mp_counts)\n",
    "\n",
    "        with open(file_loc, \"wb\") as f:\n",
    "            pickle.dump(mp_counts, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of paths populated: 69.15%\n",
      "Percent of paths populated: 74.40%\n"
     ]
    }
   ],
   "source": [
    "for mp_counts in mp_counts_ls3:\n",
    "    print(\n",
    "        f\"Percent of paths populated: {(mp_counts['pair_count'] != 0).sum() / len(mp_counts):1.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All training negatives metapath counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28,791 Compounds * 8,258 Diseases = 237,756,078 C-D Pairs\n",
      "237,756,078 C-D Pairs * 8,198 Metapaths = 1,949,124,327,444 Matrix Values\n",
      "7,261.1 GB of matrix values\n",
      "0.886 GB per metapath\n",
      "28,794 Compounds * 8,262 Diseases = 237,896,028 C-D Pairs\n",
      "237,896,028 C-D Pairs * 8,198 Metapaths = 1,950,271,637,544 Matrix Values\n",
      "7,265.3 GB of matrix values\n",
      "0.886 GB per metapath\n"
     ]
    }
   ],
   "source": [
    "# All Training Negatives\n",
    "# Get potential training negatives as compounds or diseases with no known treatments\n",
    "pos_comps_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\" | type in @comp_group')[\"start_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "pos_dis_ls = [\n",
    "    set(edges.query('type == \"treats_CtD\" | type in @dis_group')[\"end_id\"])\n",
    "    for edges in edges_ls\n",
    "]\n",
    "\n",
    "neg_comps_ls = [\n",
    "    set(nodes.query('label == \"ChemicalSubstance\"')[\"id\"]) - pos_comps_ls[i]\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "neg_dis_ls = [\n",
    "    set(nodes.query('label == \"Disease\"')[\"id\"]) - pos_dis_ls[i]\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "\n",
    "for i, neg_comps in enumerate(neg_comps_ls):\n",
    "    print_mem_info(len(neg_comps), len(neg_dis_ls[i]), len(non_sim_mps_ls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting negative pair counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [59:04<00:00, 40.28s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [53:26<00:00, 42.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This can take like 2 hours\n",
    "print(\"Extracting negative pair counts ...\")\n",
    "\n",
    "mp_counts_ls4 = list()\n",
    "for i, mg in enumerate(mg_ls):\n",
    "    file_loc = os.path.join(\"./path-based/rephetio/0_data\", f\"mp_counts3_{i}.pkl\")\n",
    "\n",
    "    if os.path.exists(file_loc):\n",
    "        with open(file_loc, \"rb\") as f:\n",
    "            mp_counts = pickle.load(f)\n",
    "\n",
    "        mp_counts_ls.append(mp_counts)\n",
    "\n",
    "    else:\n",
    "        mp_counts = piecewise_extraction(\n",
    "            function=mg.extract_metapath_pair_counts,\n",
    "            to_split=\"metapaths\",\n",
    "            block_size=350,\n",
    "            metapaths=[mp.abbrev for mp in mps_ls[i]],\n",
    "            start_nodes=list(neg_comps_ls[i]),\n",
    "            end_nodes=list(neg_dis_ls[i]),\n",
    "            n_jobs=30,\n",
    "        )\n",
    "        mp_counts[\"subset\"] = \"negative_pairs\"\n",
    "        mp_counts[\"frac\"] = mp_counts[\"pair_count\"] / (\n",
    "            len(keep_comps_ls[i]) * len(keep_dis_ls[i])\n",
    "        )\n",
    "        mp_counts_ls4.append(mp_counts)\n",
    "\n",
    "        with open(file_loc, \"wb\") as f:\n",
    "            pickle.dump(mp_counts, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a snapshot of all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sim_mp_names_ls = [\n",
    "    [mp.abbrev for mp in non_sim_mps] for non_sim_mps in non_sim_mps_ls\n",
    "]\n",
    "\n",
    "all_mp_counts_ls, no_sim_mp_counts_ls, sim_mp_counts_ls = list(), list(), list()\n",
    "for i, mp_counts in enumerate(mp_counts_ls):\n",
    "    all_mp_counts = pd.concat(\n",
    "        [mp_counts, mp_counts_ls2[i], mp_counts_ls3[i], mp_counts_ls4[i]],\n",
    "        sort=False,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    # all_mp_counts = pd.concat(all_mp_counts, sort=False, ignore_index=True)\n",
    "    all_mp_counts[\"sim_mp\"] = all_mp_counts[\"mp\"].apply(\n",
    "        lambda m: m not in non_sim_mp_names_ls[i]\n",
    "    )\n",
    "\n",
    "    no_sim_mp_counts = all_mp_counts.query(\"sim_mp == False\")\n",
    "    sim_mp_counts = all_mp_counts.query(\"sim_mp == True\")\n",
    "\n",
    "    all_mp_counts_ls.append(all_mp_counts)\n",
    "    no_sim_mp_counts_ls.append(no_sim_mp_counts)\n",
    "    sim_mp_counts_ls.append(sim_mp_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the metapath counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mp_counts_ls[0].to_csv(\n",
    "    \"./path-based/rephetio/0_data/test/all_mp_counts.csv\", index=False\n",
    ")\n",
    "all_mp_counts_ls[1].to_csv(\n",
    "    \"./path-based/rephetio/0_data/valid/all_mp_counts.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "* Optimize Degree weighted path count hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import valid data for training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout is valid\n",
    "edges_no_valid = pd.read_csv(\"./data/remain_edges_no-valid.csv\", dtype=str)\n",
    "edges_valid = pd.read_csv(\"./data/holdout_edges_valid.csv\")\n",
    "\n",
    "# holdout is test\n",
    "edges_no_test = pd.read_csv(\"./data/remain_edges_no-test.csv\", dtype=str)\n",
    "edges_test = pd.read_csv(\"./data/holdout_edges_test.csv\")\n",
    "nodes = pd.read_csv(\"./data/nodes_rep.csv\")\n",
    "\n",
    "# add to a list\n",
    "nodes_ls = [nodes, nodes]\n",
    "edges_ls = [edges_no_valid, edges_no_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the graph formatted as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing node and edge data...\n",
      "Initializing metagraph...\n",
      "Generating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 74/74 [00:53<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining degrees for each node and metaedge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 74/74 [00:25<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighting matrices by degree with dampening factor 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 74.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing node and edge data...\n",
      "Initializing metagraph...\n",
      "Generating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 75/75 [00:54<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining degrees for each node and metaedge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 75/75 [00:25<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighting matrices by degree with dampening factor 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 82.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the matrix formatted graph\n",
    "mg_ls = list()\n",
    "mg = MatrixFormattedGraph(\n",
    "    nodes,\n",
    "    edges_no_valid,\n",
    "    \"ChemicalSubstance\",\n",
    "    \"Disease\",\n",
    "    max_length=4,\n",
    "    w=0.4,\n",
    "    n_jobs=30,\n",
    ")\n",
    "\n",
    "mg_ls.append(mg)\n",
    "\n",
    "mg = MatrixFormattedGraph(\n",
    "    nodes,\n",
    "    edges_no_test,\n",
    "    \"ChemicalSubstance\",\n",
    "    \"Disease\",\n",
    "    max_length=4,\n",
    "    w=0.4,\n",
    "    n_jobs=30,\n",
    ")\n",
    "\n",
    "mg_ls.append(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse, csc_matrix, csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from hetnet_ml.extractor import piecewise_extraction\n",
    "from data_tools.ml import get_model_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformer\n",
    "* An inverse hyperbolic sine transformation is used to transfomr the features\n",
    "\n",
    "* The transormation is thus:\n",
    "\n",
    "$$sinh^{-1} \\left( \\frac{X_{mp}}{\\sigma_{mp}} \\right)$$\n",
    "\n",
    "where $X_{mp}$ is the column in the dwpc feature matrix $X$ coreesponding to metapath $mp$ and $\\sigma_{mp}$ is the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanScaledArcsinhTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            self.initial_mean_ = X.tocoo().tocsc().mean(axis=0).A[0]\n",
    "        else:\n",
    "            self.initial_mean_ = X.mean(axis=0)\n",
    "\n",
    "        # If input was DataFrame, Converts resultant series to ndarray\n",
    "        try:\n",
    "            self.initial_mean_ = self.initial_mean_.values\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # If inital mean == 0, likely all values were zero\n",
    "        # this prevents issues later.\n",
    "        self.initial_mean_[np.where(self.initial_mean_ == 0.0)] = 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return np.arcsinh(X.tocoo().tocsc().multiply(self.initial_mean_**-1))\n",
    "        return np.arcsinh(X / self.initial_mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homegrown feature selector\n",
    "\n",
    "Runs 6 analysis on the traning data to select features.\n",
    "\n",
    "1. Correlation to the output\n",
    "2. Chi_squared test\n",
    "3. Recursive Feature Elimination with a Ridge Regressor\n",
    "4. Embedded Feature Selection from a Lasso Regressor\n",
    "5. Embedded Feature Selection from a Randomn Forest Classifier\n",
    "6. Embedded Feature Selection from a Gradient Boosting Classifier\n",
    "\n",
    "Each analysis will select `num_feats` best features. The selected features will then by chosen via a voting method with `min_selections` out of the 6 elements required to for a feature to be kept. \n",
    "\n",
    "\n",
    "We have also added an option for `always_keep`:  This allows for domain expertise to be factored into the feature selection process.  In our case, we know some metapaths are specifically mechanistic, so we want to include those wherever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y, feature_names, num_feats):\n",
    "    cor_list = []\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in range(X.shape[1]):\n",
    "        if issparse(X):\n",
    "            x = X[:, i].A.reshape(len(y))\n",
    "        else:\n",
    "            x = X[:, i]\n",
    "        cor = np.corrcoef(x, y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = np.array(feature_names)[\n",
    "        np.argsort(np.abs(cor_list))[-num_feats:].tolist()\n",
    "    ].tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    return [True if i in cor_feature else False for i in feature_names]\n",
    "\n",
    "\n",
    "def chi2_selector(X, y, num_feats):\n",
    "    this_selector = SelectKBest(chi2, k=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "def rfe_selector(X, y, num_feats, random_state=None):\n",
    "    this_selector = RFE(\n",
    "        estimator=LogisticRegression(\n",
    "            C=0.1, solver=\"liblinear\", random_state=random_state\n",
    "        ),\n",
    "        n_features_to_select=num_feats,\n",
    "        step=0.2,\n",
    "        verbose=5,\n",
    "    )\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "def embeded_lr_selector(X, y, num_feats, random_state=None):\n",
    "    this_selector = SelectFromModel(\n",
    "        LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=random_state),\n",
    "        max_features=num_feats,\n",
    "    )\n",
    "    this_selector.fit(X, y)\n",
    "\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "def embeded_rf_selector(X, y, num_feats, n_jobs, random_state=None):\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=50, n_jobs=n_jobs, random_state=random_state\n",
    "    )\n",
    "    this_selector = SelectFromModel(rfc, max_features=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "def embeded_xgb_selector(X, y, num_feats, n_jobs=1, random_state=None):\n",
    "    # XGBoost takes 0 as default random state\n",
    "    if random_state is None:\n",
    "        random_state = 0\n",
    "    # Paramaters optimized for speed, rather than accuracy (as we have 5 other estimators also providing votes)\n",
    "    xgbc = XGBClassifier(\n",
    "        max_depth=5,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.16,\n",
    "        min_child_weight=1,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    this_selector = SelectFromModel(xgbc, max_features=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=100,\n",
    "        min_selections=4,\n",
    "        n_jobs=1,\n",
    "        feature_names=None,\n",
    "        always_keep=None,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.num_features = num_features\n",
    "        self.min_selections = min_selections\n",
    "        self.n_jobs = n_jobs\n",
    "        self.feature_names = feature_names\n",
    "        self.always_keep = always_keep\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        X_norm = MaxAbsScaler().fit_transform(X)\n",
    "        if issparse(X):\n",
    "            if type(X) != csc_matrix:\n",
    "                X = X.tocsc()\n",
    "            X_norm = X_norm.tocsc()\n",
    "\n",
    "        print(\"Running Cor\")\n",
    "        cor_support = cor_selector(X, y, self.feature_names, self.num_features)\n",
    "        print(\"Running Chi2\")\n",
    "        chi_support = chi2_selector(X_norm, y, self.num_features)\n",
    "        print(\"Running RFE\")\n",
    "        rfe_support = rfe_selector(X_norm, y, self.num_features, self.random_state)\n",
    "        print(\"Running LR\")\n",
    "        embeded_lr_support = embeded_lr_selector(\n",
    "            X_norm, y, self.num_features, self.random_state\n",
    "        )\n",
    "        print(\"Running RF\")\n",
    "        embeded_rf_support = embeded_rf_selector(\n",
    "            X, y, self.num_features, n_jobs=self.n_jobs, random_state=self.random_state\n",
    "        )\n",
    "        print(\"Running XG\")\n",
    "        embeded_xgb_support = embeded_xgb_selector(\n",
    "            X, y, self.num_features, n_jobs=self.n_jobs, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        feature_selection_df = pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": self.feature_names,\n",
    "                \"pearson\": cor_support,\n",
    "                \"chi_2\": chi_support,\n",
    "                \"rfe\": rfe_support,\n",
    "                \"logistics\": embeded_lr_support,\n",
    "                \"random_forest\": embeded_rf_support,\n",
    "                \"xgboost\": embeded_xgb_support,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        feature_selection_df[\"total\"] = np.sum(feature_selection_df.iloc[:, 1:], axis=1)\n",
    "        self.feature_selection_df_ = feature_selection_df\n",
    "\n",
    "        keep_features = feature_selection_df.query(\n",
    "            \"total >= {}\".format(self.min_selections)\n",
    "        )[\"feature\"].tolist()\n",
    "\n",
    "        # Keep the features that we always want (e.g. domain expertise)\n",
    "        if self.always_keep is not None:\n",
    "            keep_features.extend(self.always_keep)\n",
    "\n",
    "        self.keep_features_ = [f for f in self.feature_names if f in keep_features]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        if issparse(X) and type(X) != csc_matrix:\n",
    "            X = X.tocsc()\n",
    "        return X[\n",
    "            :, [i for i, f in enumerate(self.feature_names) if f in self.keep_features_]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the types that are relevant\n",
    "comp_group = [\n",
    "    \"indication_CiD\",\n",
    "    \"indication_CiG\",\n",
    "    \"indication_CiP\",\n",
    "    \"indication_CiPW\",\n",
    "    \"treats_CtP\",\n",
    "    \"treats_CtD\",\n",
    "]\n",
    "\n",
    "dis_group = [\n",
    "    \"treats_GtD\",\n",
    "    \"treats_CtD\",\n",
    "    \"indication_AiD\",\n",
    "    \"indication_CiD\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### further split the dataset missing the valid/test indications\n",
    "* If you recall the dataset was split by 80/10/10 by train/valid/test\n",
    "* the 80% is further separated by 85/15% for HPO\n",
    "* finally the evaluation will occur on the remaining 10% for valid/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.15\n",
    "rs = 20200123\n",
    "\n",
    "treat_comps_ls = [\n",
    "    set(edges.query(\"type in @comp_group\")[\"start_id\"]) for edges in edges_ls\n",
    "]\n",
    "# Sample the negatives and subsample\n",
    "keep_comps_ls = [\n",
    "    set(\n",
    "        nodes.query(\n",
    "            'id not in @treat_comps_ls[@i] and label == \"ChemicalSubstance\"'\n",
    "        ).sample(frac=train_frac * 0.01, random_state=rs)[\"id\"]\n",
    "    )\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "# Then subsample the positives\n",
    "keep_comps_ls = [\n",
    "    keep_comps_ls[i]\n",
    "    | set(\n",
    "        nodes.query(\"id in @treat_comps_ls[@i]\").sample(\n",
    "            frac=train_frac, random_state=rs + 1\n",
    "        )[\"id\"]\n",
    "    )\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "\n",
    "treat_dis_ls = [set(edges.query(\"type in @dis_group\")[\"end_id\"]) for edges in edges_ls]\n",
    "# Sample the negatives and subsample cv\n",
    "keep_dis_ls = [\n",
    "    set(\n",
    "        nodes.query('label == \"Disease\" and id not in @treat_dis_ls[@i]').sample(\n",
    "            frac=train_frac * 0.01, random_state=rs + 2\n",
    "        )[\"id\"]\n",
    "    )\n",
    "    for i, nodes in enumerate(nodes_ls)\n",
    "]\n",
    "# Take the diseases Treated by these compounds\n",
    "keep_dis_ls = [\n",
    "    keep_dis_ls[i]\n",
    "    | set(\n",
    "        edges.query(\"type in @dis_group and start_id in @keep_comps_ls[@i]\")[\"end_id\"]\n",
    "    )\n",
    "    for i, edges in enumerate(edges_ls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mp</th>\n",
       "      <th>pair_count</th>\n",
       "      <th>subset</th>\n",
       "      <th>frac</th>\n",
       "      <th>sim_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CpoAawD</td>\n",
       "      <td>5854758</td>\n",
       "      <td>all_pairs</td>\n",
       "      <td>2.017724e+06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CpoAiD</td>\n",
       "      <td>0</td>\n",
       "      <td>all_pairs</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mp  pair_count     subset          frac  sim_mp\n",
       "0  CpoAawD     5854758  all_pairs  2.017724e+06   False\n",
       "1   CpoAiD           0  all_pairs  0.000000e+00   False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MP counts will be the same with or without weights, so just use the original\n",
    "save_folder = \"./path-based/rephetio/0_data\"\n",
    "all_mp_counts_ls = [\n",
    "    pd.read_csv(os.path.join(save_folder, \"valid\", \"all_mp_counts.csv\")),\n",
    "    pd.read_csv(os.path.join(save_folder, \"test\", \"all_mp_counts.csv\")),\n",
    "]\n",
    "\n",
    "all_mp_counts_ls[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the memory required to extract all compound-disease pairs\n",
    "float_size = 32  # bits\n",
    "bits_per_gb = 8589934592\n",
    "\n",
    "\n",
    "def print_mem_info(n_comp, n_dis, n_mps):\n",
    "    print(f\"{n_comp:,} Compounds * {n_dis:,} Diseases = {n_comp * n_dis:,} C-D Pairs\")\n",
    "    print(\n",
    "        f\"{n_comp * n_dis:,} C-D Pairs * {n_mps:,} Metapaths = {n_comp * n_dis * n_mps:,} Matrix Values\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{n_comp * n_dis * n_mps * float_size / (bits_per_gb):1,.1f} GB of matrix values\"\n",
    "    )\n",
    "    print(f\"{n_comp * n_dis * float_size / (bits_per_gb):1,.3f} GB per metapath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./path-based/rephetio/0_data/valid\n",
      "1,792 Compounds * 3,123 Diseases = 5,596,416 C-D Pairs\n",
      "5,596,416 C-D Pairs * 6,392 Metapaths = 35,772,291,072 Matrix Values\n",
      "133.3 GB of matrix values\n",
      "0.021 GB per metapath\n",
      "./path-based/rephetio/0_data/test\n",
      "1,793 Compounds * 3,195 Diseases = 5,728,635 C-D Pairs\n",
      "5,728,635 C-D Pairs * 5,814 Metapaths = 33,306,283,890 Matrix Values\n",
      "124.1 GB of matrix values\n",
      "0.021 GB per metapath\n"
     ]
    }
   ],
   "source": [
    "non_sim_names_ls = [\n",
    "    all_mp_counts.query(\"sim_mp == False\")[\"mp\"].unique().tolist()\n",
    "    for all_mp_counts in all_mp_counts_ls\n",
    "]\n",
    "mp_qr_ls = [\n",
    "    all_mp_counts.query(\n",
    "        'subset == \"all_pairs\" and mp in @non_sim_names_ls[@i] and pair_count > 0'\n",
    "    )\n",
    "    for i, all_mp_counts in enumerate(all_mp_counts_ls)\n",
    "]\n",
    "good_mps_ls = [mp_qr[\"mp\"].tolist() for mp_qr in mp_qr_ls]\n",
    "\n",
    "save_folder_ls = [os.path.join(save_folder, \"valid\"), os.path.join(save_folder, \"test\")]\n",
    "for i, keep_comps in enumerate(keep_comps_ls):\n",
    "    print(f\"{save_folder_ls[i]}\")\n",
    "    print_mem_info(len(keep_comps), len(keep_dis_ls[i]), len(good_mps_ls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,948 Positive training examples in subset\n",
      "11,167 Positive training examples in subset\n"
     ]
    }
   ],
   "source": [
    "for i, edges in enumerate(edges_ls):\n",
    "    print(\n",
    "        f\"\"\"{len(edges.query(\"start_id in @keep_comps_ls[@i] and (type == 'indication_CiD' | type =='treats_CtD')\")):,} Positive training examples in subset\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in known metapaths from DrugMechDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mech_mps = pd.read_csv(\"./path-based/rephetio/0_data/manual/mech_mps.txt\", header=None)[\n",
    "    0\n",
    "].values\n",
    "dmdb_feat_ls = [\n",
    "    set(\n",
    "        all_mp_counts.query(\n",
    "            'mp in @mech_mps and subset == \"all_pairs\" and pair_count > 0 and sim_mp == False'\n",
    "        )[\"mp\"]\n",
    "    )\n",
    "    for all_mp_counts in all_mp_counts_ls\n",
    "]\n",
    "\n",
    "for dmdb_feat in dmdb_feat_ls:\n",
    "    len(dmdb_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the features\n",
    "Use the pair counts to sort metapath extractions as a naive load balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_mps_for_pw_extraction(n_big_calcs, big_per_block, mp_list, frac_info):\n",
    "\n",
    "    big_mp = (\n",
    "        frac_info.sort_values(\"frac\", ascending=False).head(n_big_calcs)[\"mp\"].tolist()\n",
    "    )\n",
    "    other_mp = list(set(mp_list) - set(big_mp))\n",
    "\n",
    "    block_size = len(other_mp) // (len(big_mp) // big_per_block)\n",
    "    n_blocks = (len(big_mp) + len(other_mp)) // block_size\n",
    "\n",
    "    out = []\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(big_per_block):\n",
    "            idx = i * big_per_block + j\n",
    "            out.append(big_mp[idx])\n",
    "        out += other_mp[i * block_size : (i + 1) * block_size]\n",
    "\n",
    "    out += list(set(other_mp) - set(out))\n",
    "\n",
    "    return out, block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30, 5 finishes < 30 min. <- OVERFLOW\n",
    "# 40, 8 finishes 18min 46s <- OVERFLOW\n",
    "\n",
    "to_xtract_ls, block_size_ls = list(), list()\n",
    "for i, good_mps in enumerate(good_mps_ls):\n",
    "    to_xtract, block_size = sort_mps_for_pw_extraction(100, 5, good_mps, mp_qr_ls[i])\n",
    "    to_xtract_ls.append(to_xtract)\n",
    "    block_size_ls.append(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5814, 285)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_xtract_ls[0]), block_size_ls[0]\n",
    "len(to_xtract_ls[1]), block_size_ls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metapaths to do some prep work\n",
    "pairs_ls, feats_ls, test_dwpc_ls = list(), list(), list()\n",
    "for save_folder in save_folder_ls:\n",
    "\n",
    "    # get locations of each file\n",
    "    pairs_loc = os.path.join(save_folder, \"pairs.pkl\")\n",
    "    feats_loc = os.path.join(save_folder, \"feats.pkl\")\n",
    "    test_dwpc_loc = os.path.join(save_folder, \"test_dwpc.pkl\")\n",
    "\n",
    "    # import files\n",
    "    if (\n",
    "        os.path.exists(pairs_loc)\n",
    "        and os.path.exists(feats_loc)\n",
    "        and os.path.exists(test_dwpc_loc)\n",
    "    ):\n",
    "        with open(pairs_loc, \"rb\") as f:\n",
    "            pairs = pickle.load(f)\n",
    "        with open(feats_loc, \"rb\") as f:\n",
    "            feats = pickle.load(f)\n",
    "        with open(test_dwpc_loc, \"rb\") as f:\n",
    "            test_dwpc = pickle.load(f)\n",
    "        pairs_ls.append(pairs)\n",
    "        feats_ls.append(feats)\n",
    "        test_dwpc_ls.append(test_dwpc)\n",
    "\n",
    "    else:\n",
    "        for i, mg in enumerate(mg_ls):\n",
    "            (pairs, feats), test_dwpc = piecewise_extraction(\n",
    "                function=mg.extract_dwpc,\n",
    "                to_split=\"metapaths\",\n",
    "                block_size=block_size_ls[i],\n",
    "                axis=1,\n",
    "                metapaths=to_xtract_ls[i],\n",
    "                start_nodes=list(keep_comps_ls[i]),\n",
    "                end_nodes=list(keep_dis_ls[i]),\n",
    "                return_sparse=True,\n",
    "                sparse_df=False,\n",
    "                n_jobs=4,\n",
    "            )\n",
    "\n",
    "            pairs_ls.append(pairs)\n",
    "            feats_ls.append(feats)\n",
    "            test_dwpc_ls.append(test_dwpc)\n",
    "\n",
    "            with open(pairs_loc, \"wb\") as f:\n",
    "                pickle.dump(pairs, f)\n",
    "            with open(feats_loc, \"wb\") as f:\n",
    "                pickle.dump(feats, f)\n",
    "            with open(test_dwpc_loc, \"wb\") as f:\n",
    "                pickle.dump(test_dwpc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHEBI:100147', 'DOID:0050400'), ('CHEBI:100147', 'DOID:13148')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tups_ls = [\n",
    "    edges.query('type == \"indication_CiD\" | type == \"treats_CtD\"')[\n",
    "        [\"start_id\", \"end_id\"]\n",
    "    ]\n",
    "    .apply(tuple, axis=1)\n",
    "    .tolist()\n",
    "    for edges in edges_ls\n",
    "]\n",
    "\n",
    "pos_tups_ls[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tups_ls = [set(pos_tups) for pos_tups in pos_tups_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/5596416 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5596416/5596416 [00:04<00:00, 1122911.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chemical to Disease Pairs: 5596416\n",
      "length: 5596416\n",
      "total: 10939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5596416/5596416 [00:04<00:00, 1127626.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chemical to Disease Pairs: 5596416\n",
      "length: 5596416\n",
      "total: 10924\n"
     ]
    }
   ],
   "source": [
    "y_ls = list()\n",
    "for i, pairs in enumerate(pairs_ls):\n",
    "    y = list()\n",
    "\n",
    "    for row in tqdm(pairs.itertuples(), total=len(pairs)):\n",
    "        if set([(row.chemicalsubstance_id, row.disease_id)]) & pos_tups_ls[i]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    y = np.array(y)\n",
    "\n",
    "    pairs[\"status\"] = y\n",
    "\n",
    "    print(\n",
    "        f\"Total Chemical to Disease Pairs: {len(pairs)}\\nlength: {len(y)}\\ntotal: {sum(y)}\"\n",
    "    )\n",
    "\n",
    "    pairs_ls[i] = pairs\n",
    "    y_ls.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the potential training examples\n",
    "\n",
    "* Our class is so imbalanced, to get a sizeable number of positive training examples, we end up with many orders of magnitude more negative examples.  Many of those examples will have no connections from the compound to the disease of interest, providing a zero row in the matrix. We will not waste time training on those values, and instead focus on the ones that distinguish the positive from the negative training examples.\n",
    "\n",
    "* As this is just hyperparameter tunings, to speed things up, we will also limit the positive examples to a small portion of the negative examples, 100x (2 orders of magnitude) larger than the number of postitives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows that are Non-zero\n",
    "nz_index_ls = [\n",
    "    pairs[test_dwpc_ls[i].getnnz(1) > 0].index for i, pairs in enumerate(pairs_ls)\n",
    "]\n",
    "\n",
    "# have the number of postivies to get 100x this for the negatives.\n",
    "n_pos_ls = [pairs[\"status\"].sum() for pairs in pairs_ls]\n",
    "\n",
    "# Sample the nonzero negative examples at a rate of 100x the positive samples\n",
    "neg_index_ls = [\n",
    "    pairs.loc[nz_index_ls[i]]\n",
    "    .query(\"status == 0\")\n",
    "    .sample(n=100 * n_pos_ls[i], random_state=rs + 10)\n",
    "    .sort_index()\n",
    "    .index\n",
    "    for i, pairs in enumerate(pairs_ls)\n",
    "]\n",
    "\n",
    "# and of course take the training postivies\n",
    "pos_index_ls = [pairs.query(\"status == 1\").index for pairs in pairs_ls]\n",
    "\n",
    "# Union the two\n",
    "train_index_ls = [\n",
    "    pos_index.union(neg_index_ls[i]) for i, pos_index in enumerate(pos_index_ls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_ls = [np.array(feats) for feats in feats_ls]\n",
    "nz_feats_ls = [feats[test_dwpc_ls[i].getnnz(0) > 0] for i, feats in enumerate(feats_ls)]\n",
    "feat_index_ls = [test_dwpc.getnnz(0) > 0 for test_dwpc in test_dwpc_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our compounds as ndarrays for easy indexing with sklearns StratifiedKFold\n",
    "keep_comps_ls = [np.array(list(keep_comps)) for keep_comps in keep_comps_ls]\n",
    "# Need to know how to properly stratify the split\n",
    "is_treat_comp_ls = [\n",
    "    np.array([1 if c in treat_comps else 0 for c in keep_comps_ls[i]])\n",
    "    for i, treat_comps in enumerate(treat_comps_ls)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the features\n",
    "This is a time consuming and expensive step.\n",
    "We will do it once with the initial DWPC for parameter tuning and perform it again after parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "msat = MeanScaledArcsinhTransformer()\n",
    "trans_dwpc_ls = [\n",
    "    msat.fit_transform(test_dwpc[train_index_ls[i]][:, feat_index_ls[i]])\n",
    "    for i, test_dwpc in enumerate(test_dwpc_ls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for save_folder in save_folder_ls:\n",
    "    tmp_dir = os.path.join(save_folder, \"tmp\")\n",
    "    try:\n",
    "        os.mkdir(tmp_dir)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features_ls = list()\n",
    "\n",
    "for i, save_folder in enumerate(save_folder_ls):\n",
    "    tmp_dir = os.path.join(save_folder, \"tmp\")\n",
    "    if os.path.exists(os.path.join(tmp_dir, \"test_feats.txt\")):\n",
    "        keep_features = pd.read_csv(os.path.join(tmp_dir, \"test_feats.txt\"))[\n",
    "            \"0\"\n",
    "        ].tolist()\n",
    "    else:\n",
    "        fsel = FeatureSelector(\n",
    "            num_features=500,\n",
    "            min_selections=4,\n",
    "            n_jobs=30,\n",
    "            feature_names=nz_feats_ls[i].tolist(),\n",
    "            always_keep=dmdb_feat_ls[i],\n",
    "            random_state=rs + 5,\n",
    "            # save=save_folder,\n",
    "        )\n",
    "        sel_dwpc = fsel.fit_transform(trans_dwpc_ls[i], y[train_index_ls[i]])\n",
    "        keep_features = fsel.keep_features_\n",
    "        pd.Series(keep_features).to_csv(\n",
    "            os.path.join(tmp_dir, \"test_feats.txt\"), index=False\n",
    "        )\n",
    "        fsel.feature_selection_df_.to_csv(\n",
    "            os.path.join(tmp_dir, \"test_fs_df.csv\"), index=False\n",
    "        )\n",
    "    keep_features_ls.append(keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dwpc = dict()\n",
    "\n",
    "\n",
    "def get_dwpc(w, i):\n",
    "    global all_dwpc\n",
    "\n",
    "    # only extract if not previously extracted\n",
    "    if w not in all_dwpc.keys():\n",
    "        mg.update_w(w)\n",
    "        (pairs, feats), dwpc = mg.extract_dwpc(\n",
    "            metapaths=keep_features,\n",
    "            start_nodes=list(keep_comps_ls[i]),\n",
    "            end_nodes=list(keep_dis_ls[i]),\n",
    "            return_sparse=True,\n",
    "            sparse_df=False,\n",
    "            n_jobs=4,\n",
    "        )\n",
    "        # next step is split, so need sparse rows\n",
    "        dwpc = dwpc.tocoo().tocsr()\n",
    "        # all_dwpc[w] = dwpc\n",
    "        return dwpc\n",
    "    else:\n",
    "        return all_dwpc[w]\n",
    "\n",
    "\n",
    "def neg_log_scal(val):\n",
    "    \"\"\"Scale values logrithmicly. Larger input yields smaller result\"\"\"\n",
    "    return 1 - np.log1p(val)\n",
    "\n",
    "\n",
    "def pos_log_scal(val):\n",
    "    \"\"\"Scale values logrithmicly. Larger input yields larger result\"\"\"\n",
    "    return np.log1p(val)\n",
    "\n",
    "\n",
    "def auroc_mean_loss(auroc_mean, strength=2, scal_factor=0.5):\n",
    "    mean_roc_shift = (np.abs(auroc_mean - 0.5)) / scal_factor\n",
    "    baseline_roc = neg_log_scal(1)\n",
    "    return (neg_log_scal(mean_roc_shift) - baseline_roc) * strength\n",
    "\n",
    "\n",
    "def avg_pre_mean_loss(avg_pre_mean, strength=2.5, scal_factor=0.15):\n",
    "    avg_pre_mean_scal = avg_pre_mean / scal_factor\n",
    "    baseline_prc = neg_log_scal(1)\n",
    "    return (neg_log_scal(avg_pre_mean_scal) - baseline_prc) * strength\n",
    "\n",
    "\n",
    "def auroc_std_loss(auroc_std, strength=2):\n",
    "    return pos_log_scal(auroc_std) * strength\n",
    "\n",
    "\n",
    "def avg_pre_std_loss(avg_pre_std, strength=2):\n",
    "    return pos_log_scal(avg_pre_std) * strength\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def calc_loss(auroc_mean, avg_pre_mean, auroc_std, avg_pre_std):\n",
    "    mean_roc_loss = auroc_mean_loss(auroc_mean)\n",
    "    mean_pre_loss = avg_pre_mean_loss(avg_pre_mean)\n",
    "    std_roc_loss = auroc_std_loss(auroc_std)\n",
    "    std_pre_loss = avg_pre_std_loss(avg_pre_std)\n",
    "\n",
    "    return sigmoid(mean_roc_loss + mean_pre_loss + std_roc_loss + std_pre_loss) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt(param_space, y, num_eval, i):\n",
    "    def objective_function(params):\n",
    "        dwpc_params = {\n",
    "            k.split(\"__\")[1]: v for k, v in params.items() if k.split(\"__\")[0] == \"dwpc\"\n",
    "        }\n",
    "        enet_params = {\n",
    "            k.split(\"__\")[1]: v for k, v in params.items() if k.split(\"__\")[0] == \"enet\"\n",
    "        }\n",
    "\n",
    "        this_w = dwpc_params[\"w\"]\n",
    "\n",
    "        # Set up the post feature extraction pipeline\n",
    "        post_extraction_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"transformer\", MeanScaledArcsinhTransformer()),\n",
    "                (\"maxabs_scale\", MaxAbsScaler()),\n",
    "                (\n",
    "                    \"e_net\",\n",
    "                    LogisticRegression(\n",
    "                        penalty=\"elasticnet\",\n",
    "                        solver=\"saga\",\n",
    "                        max_iter=100,\n",
    "                        **enet_params,\n",
    "                        random_state=rs + 6,\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        ## Get the dwpc information for the current pairs\n",
    "        dwpc = get_dwpc(this_w, i)\n",
    "        this_dwpc = dwpc[train_index_ls[i]]\n",
    "\n",
    "        cv = cross_validate(\n",
    "            post_extraction_pipeline,\n",
    "            this_dwpc,\n",
    "            y,\n",
    "            cv=5,\n",
    "            scoring=[\"average_precision\", \"roc_auc\"],\n",
    "            return_estimator=True,\n",
    "        )\n",
    "\n",
    "        # Write out scores for each run\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                tmp_dir,\n",
    "                f\"scores_w_{this_w:1.4f}_C_{enet_params['C']:1.5f}_l1_{enet_params['l1_ratio']:1.4f}.txt\",\n",
    "            ),\n",
    "            \"w\",\n",
    "        ) as f_out:\n",
    "\n",
    "            f_out.write(\", \".join([str(s) for s in cv[\"test_average_precision\"]]))\n",
    "            f_out.write(\"\\n\")\n",
    "            f_out.write(\", \".join([str(s) for s in cv[\"test_roc_auc\"]]))\n",
    "            f_out.write(\"\\n\")\n",
    "\n",
    "        auroc_mean = cv[\"test_roc_auc\"].mean()\n",
    "        avg_pre_mean = cv[\"test_average_precision\"].mean()\n",
    "        auroc_std = cv[\"test_roc_auc\"].std()\n",
    "        avg_pre_std = cv[\"test_average_precision\"].std()\n",
    "\n",
    "        score = calc_loss(auroc_mean, avg_pre_mean, auroc_std, avg_pre_std)\n",
    "\n",
    "        print(\"Mean AUROC: {:1.4f}\".format(auroc_mean))\n",
    "        print(\"Mean Avg Pre: {:1.4f}\".format(avg_pre_mean))\n",
    "        print(\"STD AUROC: {:1.4f}\".format(auroc_std))\n",
    "        print(\"STD Avg Pre: {:1.4f}\".format(avg_pre_std))\n",
    "        print(\"Loss: {:1.4f}\".format(score))\n",
    "\n",
    "        return {\"loss\": score, \"status\": STATUS_OK}\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    trials = Trials()\n",
    "    # load the pickle file if you have a version saved.\n",
    "    pkl_file = os.path.join(tmp_dir, \"trials.pkl\")\n",
    "    if os.path.exists(pkl_file):\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            trials = pickle.load(f)\n",
    "\n",
    "    best_param = fmin(\n",
    "        objective_function,\n",
    "        param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_eval,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(seed=None),  # Removed fixed starting seed.\n",
    "    )\n",
    "\n",
    "    print(time() - start)\n",
    "    return trials, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous best before adding new keep_features:\n",
    "# '1l_ratio': 0.10455936193818496, 'C': 0.000556880900960339, 'w': 0.2640929485381926\n",
    "param_hyperopt = {\n",
    "    \"dwpc__w\": hp.uniform(\"w\", 0.01, 1),\n",
    "    \"enet__C\": hp.loguniform(\"C\", np.log(0.001), np.log(1)),\n",
    "    \"enet__l1_ratio\": hp.uniform(\"l1_ratio\", 0.01, 0.99),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 1100 minutes\n",
    "best_param_ls = list()\n",
    "\n",
    "# check if you already exported your trials.pkl file. if you have already, skip the time consuming step\n",
    "for i in save_folder_ls:\n",
    "    if os.path.exists(os.path.join(i, \"trials.pkl\")):\n",
    "        with open(os.path.join(i, \"trials.pkl\"), \"rb\") as f:\n",
    "            trial = pickle.load(f)\n",
    "\n",
    "        best_param_ls.append(trial.best_trial[\"misc\"][\"vals\"])\n",
    "\n",
    "if len(best_param_ls) < 5:\n",
    "    # this loop runs 50 iterations of hyperparameter optimization for each slice of cross validation\n",
    "    for i, y in enumerate(y_ls):\n",
    "        # open a temporary directory in the directory of interest to store our files in\n",
    "        tmp_dir = os.path.join(save_folder_ls[i], \"tmp\")\n",
    "\n",
    "        # incase we run out of memory, sets number of trials\n",
    "        num_eval = 50\n",
    "        num_evaluated = (\n",
    "            sum(\n",
    "                [\n",
    "                    i.startswith(\"scores\") and i.endswith(\".txt\")\n",
    "                    for i in os.listdir(tmp_dir)\n",
    "                ]\n",
    "            )\n",
    "            + 1\n",
    "        )  # needs plus 1 otherwise will start at 0\n",
    "        pkl_file = os.path.join(save_folder_ls[i], \"trials.pkl\")\n",
    "        best_param = None\n",
    "        # crappy for loop to only do 1 hpo at a time since we need to tamp down on memory limitations\n",
    "        while num_evaluated <= num_eval:\n",
    "\n",
    "            trials, best_param = hyperopt(\n",
    "                param_space=param_hyperopt,\n",
    "                y=y[train_index_ls[i]],\n",
    "                num_eval=num_evaluated,\n",
    "                i=i,\n",
    "            )\n",
    "\n",
    "            # after every iteration, dump the results\n",
    "            with open(pkl_file, \"wb\") as f:\n",
    "                pickle.dump(trials, f)\n",
    "\n",
    "            num_evaluated = (\n",
    "                sum(\n",
    "                    [\n",
    "                        file.startswith(\"scores\") and file.endswith(\".txt\")\n",
    "                        for file in os.listdir(tmp_dir)\n",
    "                    ]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "\n",
    "        best_param_ls.append(best_param)\n",
    "        with open(os.path.join(save_folder_ls[i], \"best_param.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(best_param, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./path-based/rephetio/0_data/valid\n",
      "{'C': 0.07073875743215166, 'l1_ratio': 0.014611019497691002, 'w': 0.6649765567763943}\n",
      "./path-based/rephetio/0_data/test\n",
      "{'C': 0.009084068877998714, 'l1_ratio': 0.12345440738916719, 'w': 0.9135211198916415}\n"
     ]
    }
   ],
   "source": [
    "best_param_ls = list()\n",
    "\n",
    "for i in save_folder_ls:\n",
    "    with open(os.path.join(i, \"trials.pkl\"), \"rb\") as f:\n",
    "        trial = pickle.load(f)\n",
    "\n",
    "    best_param_ls.append(trial.best_trial[\"misc\"][\"vals\"])\n",
    "\n",
    "# process best_parameters\n",
    "best_param_ls = [dict(zip(i.keys(), [j[0] for j in i.values()])) for i in best_param_ls]\n",
    "\n",
    "# return results\n",
    "for i, v in enumerate(save_folder_ls):\n",
    "    print(v)\n",
    "    print(best_param_ls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use selected parameters to do feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_params_ls = [\n",
    "    {k: v for k, v in best_param.items() if k != \"w\"} for best_param in best_param_ls\n",
    "]\n",
    "# enet_params = {k: v for k, v in best_param.items() if k != \"w\"}\n",
    "\n",
    "post_extraction_pipeline_ls = list()\n",
    "\n",
    "for i, v in enumerate(best_param_ls):  # just need a list to iterate through\n",
    "    post_extraction_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"transformer\", MeanScaledArcsinhTransformer()),\n",
    "            (\n",
    "                \"feature_selection\",\n",
    "                FeatureSelector(\n",
    "                    num_features=500,\n",
    "                    min_selections=4,\n",
    "                    n_jobs=30,\n",
    "                    feature_names=to_xtract_ls[i],\n",
    "                    always_keep=dmdb_feat_ls[i],\n",
    "                    random_state=4,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    post_extraction_pipeline_ls.append(post_extraction_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing w from 0.4 to 0.6649765567763943. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 74/74 [00:01<00:00, 48.80it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [37:28<00:00, 97.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing w from 0.6412668503071621 to 0.9135211198916415. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 75/75 [00:01<00:00, 52.78it/s]\n",
      "100%|██████████████████████████████████████████| 21/21 [36:47<00:00, 105.14s/it]\n"
     ]
    }
   ],
   "source": [
    "dwpc_ls = []\n",
    "for i, mg in enumerate(mg_ls):\n",
    "    mg.update_w(best_param_ls[i][\"w\"])\n",
    "    (a, b), dwpc = piecewise_extraction(\n",
    "        function=mg.extract_dwpc,\n",
    "        to_split=\"metapaths\",\n",
    "        block_size=block_size,\n",
    "        axis=1,\n",
    "        metapaths=to_xtract_ls[i],\n",
    "        start_nodes=list(keep_comps_ls[i]),\n",
    "        end_nodes=list(keep_dis_ls[i]),\n",
    "        return_sparse=True,\n",
    "        sparse_df=False,\n",
    "        n_jobs=30,\n",
    "    )\n",
    "    dwpc_ls.append(dwpc)\n",
    "    mg_ls[i] = mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_extraction_pipeline_ls2 = list()\n",
    "for i, post_extraction_pipeline in enumerate(post_extraction_pipeline_ls):\n",
    "    j = post_extraction_pipeline.fit(\n",
    "        dwpc_ls[i][train_index_ls[i]], y_ls[i][train_index_ls[i]]\n",
    "    )\n",
    "    post_extraction_pipeline_ls2.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_feats_ls = [i[1].keep_features_ for i in post_extraction_pipeline_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_feats: 159\n",
      "keep_features: 163\n",
      "Difference: 57\n",
      "pe_feats: 251\n",
      "keep_features: 158\n",
      "Difference: 129\n"
     ]
    }
   ],
   "source": [
    "for i, pe_feats in enumerate(pe_feats_ls):\n",
    "    print(f\"pe_feats: {len(pe_feats)}\\nkeep_features: {len(keep_features_ls[i])}\")\n",
    "    print(f\"Difference: {len(set(keep_features_ls[i])-set(pe_feats))}\")  # 39 before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, post_extraction_pipeline in enumerate(post_extraction_pipeline_ls):\n",
    "\n",
    "    # save feature selection dataframe results\n",
    "    feat_df = post_extraction_pipeline[1].feature_selection_df_\n",
    "    feat_df.sort_values(\"total\", ascending=False).to_csv(\n",
    "        os.path.join(save_folder_ls[i], \"feature_selection_df.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # pickle selected features\n",
    "    with open(os.path.join(save_folder_ls[i], \"feature_selector.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(post_extraction_pipeline[1], f)\n",
    "\n",
    "    # save kept features\n",
    "    pd.Series(post_extraction_pipeline[1].keep_features_).to_csv(\n",
    "        os.path.join(save_folder_ls[i], \"kept_features.txt\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing function arguments...\n",
      "Calculating DWPCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 159/159 [03:16<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaping Result Matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 159/159 [00:06<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking columns...\n",
      "Preparing function arguments...\n",
      "Calculating DWPCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 251/251 [02:27<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaping Result Matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 251/251 [00:13<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking columns...\n"
     ]
    }
   ],
   "source": [
    "enet_params_ls = [\n",
    "    {k: v for k, v in best_param.items() if k != \"w\"} for best_param in best_param_ls\n",
    "]\n",
    "\n",
    "post_extraction_pipeline_ls = list()\n",
    "\n",
    "for i, enet_params in enumerate(enet_params_ls):\n",
    "    # Set up the post feature extraction pipeline\n",
    "    post_extraction_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"transformer\", MeanScaledArcsinhTransformer()),\n",
    "            (\"maxabs_scale\", MaxAbsScaler()),\n",
    "            (\n",
    "                \"e_net\",\n",
    "                LogisticRegression(\n",
    "                    penalty=\"elasticnet\",\n",
    "                    solver=\"saga\",\n",
    "                    max_iter=100,\n",
    "                    **enet_params,\n",
    "                    random_state=rs + 6\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )\n",
    "    post_extraction_pipeline_ls.append(post_extraction_pipeline)\n",
    "\n",
    "dwpc_ls = list()\n",
    "for i, mg in enumerate(mg_ls):\n",
    "    ## Get the dwpc information for the current pairs\n",
    "    (stuff), dwpc = mg.extract_dwpc(\n",
    "        metapaths=pe_feats_ls[i],\n",
    "        start_nodes=list(keep_comps_ls[i]),\n",
    "        end_nodes=list(keep_dis_ls[i]),\n",
    "        return_sparse=True,\n",
    "        sparse_df=False,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    dwpc_ls.append(dwpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dwpc_ls = [dwpc[train_index_ls[i]] for i, dwpc in enumerate(dwpc_ls)]\n",
    "this_y_ls = [y[train_index_ls[i]] for i, y in enumerate(y_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ls, auroc_mean_ls, avg_pre_mean_ls, auroc_std_ls, avg_pre_std_ls, score_ls = (\n",
    "    list(),\n",
    "    list(),\n",
    "    list(),\n",
    "    list(),\n",
    "    list(),\n",
    "    list(),\n",
    ")\n",
    "\n",
    "for i, post_extraction_pipeline in enumerate(post_extraction_pipeline_ls):\n",
    "    cv = cross_validate(\n",
    "        post_extraction_pipeline,\n",
    "        this_dwpc_ls[i],\n",
    "        this_y_ls[i],\n",
    "        cv=5,\n",
    "        scoring=[\"average_precision\", \"roc_auc\"],\n",
    "        return_estimator=True,\n",
    "    )\n",
    "\n",
    "    auroc_mean = cv[\"test_roc_auc\"].mean()\n",
    "    avg_pre_mean = cv[\"test_average_precision\"].mean()\n",
    "    auroc_std = cv[\"test_roc_auc\"].std()\n",
    "    avg_pre_std = cv[\"test_average_precision\"].std()\n",
    "\n",
    "    score = calc_loss(auroc_mean, avg_pre_mean, auroc_std, avg_pre_std)\n",
    "\n",
    "    cv_ls.append(cv)\n",
    "    auroc_mean_ls.append(auroc_mean)\n",
    "    avg_pre_mean_ls.append(avg_pre_mean)\n",
    "    auroc_std_ls.append(auroc_std)\n",
    "    avg_pre_std_ls.append(avg_pre_std)\n",
    "    score_ls.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC Scores: 0.7289, 0.7823, 0.7473, 0.7942, 0.6758, \n",
      "Mean AUROC: 0.7457\n",
      "Avg Pre. Scores: 0.0902, 0.1018, 0.0940, 0.2040, 0.0738, \n",
      "Mean Avg Pre: 0.1127\n",
      "STD AUROC: 0.0421\n",
      "STD Avg Pre: 0.0465\n",
      "Loss: -0.2513\n",
      "AUROC Scores: 0.5384, 0.4580, 0.6528, 0.5433, 0.5645, \n",
      "Mean AUROC: 0.5514\n",
      "Avg Pre. Scores: 0.0116, 0.0091, 0.0295, 0.0380, 0.0121, \n",
      "Mean Avg Pre: 0.0201\n",
      "STD AUROC: 0.0623\n",
      "STD Avg Pre: 0.0115\n",
      "Loss: -0.0599\n"
     ]
    }
   ],
   "source": [
    "for i, cv in enumerate(cv_ls):\n",
    "    print((\"AUROC Scores: \" + 5 * \"{:1.4f}, \").format(*cv[\"test_roc_auc\"].tolist()))\n",
    "    print(\"Mean AUROC: {:1.4f}\".format(auroc_mean_ls[i]))\n",
    "    print(\n",
    "        (\"Avg Pre. Scores: \" + 5 * \"{:1.4f}, \").format(\n",
    "            *cv[\"test_average_precision\"].tolist()\n",
    "        )\n",
    "    )\n",
    "    print(\"Mean Avg Pre: {:1.4f}\".format(avg_pre_mean_ls[i]))\n",
    "    print(f\"STD AUROC: {auroc_std_ls[i]:1.4f}\")\n",
    "    print(f\"STD Avg Pre: {avg_pre_std_ls[i]:1.4f}\")\n",
    "    print(f\"Loss: {score_ls[i]:1.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some important variables for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hetnet_ml.extractor.MatrixFormattedGraph at 0x7f4c7caca390>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg_ls[1]  # test graph without the test set\n",
    "edges_test  # test set\n",
    "best_param_ls[1]  # best parameters\n",
    "pos_tups_ls[1]  # positive samples\n",
    "\n",
    "# ids that are referenced in the edges_no_test\n",
    "ref_c = nodes_ls[1].dropna()[\"id\"].tolist()\n",
    "\n",
    "# extract the selected features\n",
    "fs = pickle.load(open(os.path.join(save_folder_ls[1], \"feature_selector.pkl\"), \"rb\"))\n",
    "\n",
    "# metapaths to extract\n",
    "mp_to_extract = list(set(fs.keep_features_).union(set(dmdb_feat_ls[1])))\n",
    "\n",
    "# elastic net parameters\n",
    "enet_params_ls[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize logistic regression with elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_extraction_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"transformer\", MeanScaledArcsinhTransformer()),\n",
    "        (\"maxabs_scale\", MaxAbsScaler()),\n",
    "        (\n",
    "            \"e_net\",\n",
    "            LogisticRegression(\n",
    "                penalty=\"elasticnet\",\n",
    "                solver=\"saga\",\n",
    "                max_iter=500,\n",
    "                **enet_params_ls[1],\n",
    "                random_state=rs + 6\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing function arguments...\n",
      "Calculating DWPCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 251/251 [02:24<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaping Result Matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 251/251 [00:13<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking columns...\n",
      "Shaping Matricies...\n",
      "Getting Target Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 5728635/5728635 [00:05<00:00, 1088013.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "[Pipeline] ....... (step 1 of 3) Processing transformer, total=   6.0s\n",
      "[Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.2s\n",
      "[Pipeline] ............. (step 3 of 3) Processing e_net, total=  21.2s\n",
      "Getting Model Coef\n"
     ]
    }
   ],
   "source": [
    "# Check if we've already processed the files, if so, skip.\n",
    "if os.path.exists(os.path.join(save_folder_ls[1], \"coef_test.csv\")):\n",
    "    next\n",
    "\n",
    "else:\n",
    "    (pairs, out_mp), dwpc = mg_ls[1].extract_dwpc(\n",
    "        metapaths=mp_to_extract,\n",
    "        start_nodes=keep_comps_ls[1],\n",
    "        end_nodes=np.array(list(keep_dis_ls[1])),\n",
    "        return_sparse=True,\n",
    "        sparse_df=False,\n",
    "        verbose=True,\n",
    "        n_jobs=30,\n",
    "    )\n",
    "    # Split the Training and Testing\n",
    "    print(\"Shaping Matricies...\")\n",
    "    dwpc = dwpc.tocoo().tocsr()\n",
    "\n",
    "    print(\"Getting Target Values\")\n",
    "    y = []\n",
    "    for row in tqdm(pairs.itertuples(), total=len(pairs)):\n",
    "        # pos_tups defined several cells above\n",
    "        if set([(row.chemicalsubstance_id, row.disease_id)]) & pos_tups_ls[1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    y = np.array(y)\n",
    "    pairs[\"status\"] = y\n",
    "\n",
    "    # subset so that we're training only with nonzero dwpc rows\n",
    "    # Get the training examples that have metapaths\n",
    "    nz_index = pairs[dwpc.getnnz(1) > 0].index\n",
    "    # have the number of postivies to get 100x this for the negatives.\n",
    "    n_pos = pairs[\"status\"].sum()\n",
    "    # Sample the nonzero negative examples at a rate of 100x the positive samples\n",
    "    neg_index = (\n",
    "        pairs.loc[nz_index]\n",
    "        .query(\"status == 0\")\n",
    "        .sample(n=100 * n_pos, random_state=rs + 10)\n",
    "        .sort_index()\n",
    "        .index\n",
    "    )\n",
    "    # and of course take the training postivies\n",
    "    pos_index = pairs.query(\"status == 1\").index\n",
    "    # Union the two\n",
    "    train_index = pos_index.union(neg_index)\n",
    "\n",
    "    # Fit the model and get results\n",
    "    print(\"Training Model\")\n",
    "    post_extraction_pipeline.fit(dwpc[train_index, :], y[train_index])\n",
    "\n",
    "    print(\"Getting Model Coef\")\n",
    "    coef = get_model_coefs(\n",
    "        post_extraction_pipeline[-1], dwpc, mp_to_extract\n",
    "    ).sort_values(\"coef\", ascending=False)\n",
    "\n",
    "    # Save model\n",
    "    coef.to_csv(os.path.join(save_folder_ls[1], \"coef_test.csv\"), index=False)\n",
    "    pickle.dump(\n",
    "        post_extraction_pipeline,\n",
    "        open(os.path.join(save_folder_ls[1], \"model_test.pkl\"), \"wb\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass in holdout set to the generated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at the holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_id</th>\n",
       "      <th>type</th>\n",
       "      <th>end_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>indication_CiD</td>\n",
       "      <td>DOID:10763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEBI:135738</td>\n",
       "      <td>indication_CiD</td>\n",
       "      <td>DOID:10763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEBI:135876</td>\n",
       "      <td>indication_CiD</td>\n",
       "      <td>DOID:11054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEBI:135923</td>\n",
       "      <td>indication_CiD</td>\n",
       "      <td>DOID:14499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:135925</td>\n",
       "      <td>indication_CiD</td>\n",
       "      <td>DOID:1094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start_id            type      end_id\n",
       "0  CHEBI:135735  indication_CiD  DOID:10763\n",
       "1  CHEBI:135738  indication_CiD  DOID:10763\n",
       "2  CHEBI:135876  indication_CiD  DOID:11054\n",
       "3  CHEBI:135923  indication_CiD  DOID:14499\n",
       "4  CHEBI:135925  indication_CiD   DOID:1094"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_id</th>\n",
       "      <th>type</th>\n",
       "      <th>end_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>CHEBI:9667</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>CHEBI:41423</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>CHEBI:9168</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:7490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>CHEBI:41879</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>CHEBI:8378</td>\n",
       "      <td>indication_CiG</td>\n",
       "      <td>NCBIGene:367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_id            type         end_id\n",
       "532   CHEBI:9667  indication_CiG   NCBIGene:367\n",
       "533  CHEBI:41423  indication_CiG   NCBIGene:367\n",
       "534   CHEBI:9168  indication_CiG  NCBIGene:7490\n",
       "535  CHEBI:41879  indication_CiG   NCBIGene:367\n",
       "536   CHEBI:8378  indication_CiG   NCBIGene:367"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter holdout set first\n",
    "* start must be a compound\n",
    "* end must be a disease. Metatype must be of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "indication_CiD    521\n",
       "indication_CiG     11\n",
       "indication_CiP      5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the relation types\n",
    "edges_test[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only end_ids that are diseases\n",
    "edges_test_filt = edges_test.query('type == \"indication_CiD\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing function arguments...\n",
      "Calculating DWPCs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 251/251 [02:50<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaping Result Matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 251/251 [00:02<00:00, 108.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking columns...\n",
      "Getting Target Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 271441/271441 [00:00<00:00, 1139616.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Probabilities\n",
      "AUROC: 0.8038\n",
      "AUPR: 0.1054\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(save_folder_ls[1], \"results_test.csv\")):\n",
    "    next\n",
    "else:\n",
    "    (pairs, out_mp), dwpc = mg_ls[1].extract_dwpc(\n",
    "        metapaths=mp_to_extract,\n",
    "        start_nodes=edges_test_filt.start_id.to_numpy(),\n",
    "        end_nodes=edges_test_filt.end_id.to_numpy(),\n",
    "        # start_nodes=test_comps_ls[i],\n",
    "        # end_nodes=test_dis_ls[i],\n",
    "        return_sparse=True,\n",
    "        sparse_df=False,\n",
    "        verbose=True,\n",
    "        n_jobs=30,\n",
    "    )\n",
    "\n",
    "    print(\"Getting Target Values\")\n",
    "    y = []\n",
    "    for row in tqdm(pairs.itertuples(), total=len(pairs)):\n",
    "        # pos_tups defined several cells above\n",
    "        if set([(row.chemicalsubstance_id, row.disease_id)]) & pos_tups_ls[1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    # Fit the model and get results\n",
    "    print(\"Getting Probabilities\")\n",
    "    y_proba = post_extraction_pipeline.predict_proba(dwpc)[:, 1]\n",
    "\n",
    "    # Get metrics\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "    avg_prec = average_precision_score(y, y_proba)\n",
    "    print(\"AUROC: {:1.4f}\".format(roc_auc))\n",
    "    print(\"AUPR: {:1.4f}\".format(avg_prec))\n",
    "\n",
    "    pairs[\"status\"] = y\n",
    "    pairs[\"proba\"] = y_proba\n",
    "\n",
    "    # Save results\n",
    "    pairs.to_csv(os.path.join(save_folder_ls[1], \"results_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "* `status` represents whether the compound-disease pair is an indication (1) or not (0).\n",
    "* `proba` is the score of the compound-disease pair.\n",
    "* compound-disease pairs that have a lower `proba` are less likely to be true than those with a higher `proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the results back in\n",
    "results = pd.read_csv(\n",
    "    os.path.join(save_folder_ls[1], \"results_test.csv\")\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chemicalsubstance_id</th>\n",
       "      <th>disease_id</th>\n",
       "      <th>status</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>DOID:0050425</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>DOID:0050742</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>DOID:0050783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>DOID:0060224</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHEBI:135735</td>\n",
       "      <td>DOID:0080159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chemicalsubstance_id    disease_id  status     proba\n",
       "0          CHEBI:135735  DOID:0050425       0  0.007140\n",
       "4          CHEBI:135735  DOID:0050742       0  0.006813\n",
       "5          CHEBI:135735  DOID:0050783       0  0.008267\n",
       "6          CHEBI:135735  DOID:0060224       0  0.018347\n",
       "10         CHEBI:135735  DOID:0080159       0  0.006699"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a view of the dataframe\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
