{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/data_tools/df_processing.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import data_tools.graphs as gt\n",
    "from hetnet_ml.extractor import MatrixFormattedGraph, piecewise_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing node and edge data...\n",
      "Initializing metagraph...\n",
      "Generating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [02:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining degrees for each node and metaedge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:30<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighting matrices by degree with dampening factor 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:02<00:00, 42.02it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = pd.read_csv('../2_pipeline/11a_Model_Prep_Holdout_Set/out/nodes.csv', dtype=str)\n",
    "edges = pd.read_csv('../2_pipeline/11a_Model_Prep_Holdout_Set/out/edges.csv', dtype=str)\n",
    "\n",
    "mg = MatrixFormattedGraph(nodes, edges, 'ChemicalSubstance', 'Disease', max_length=4, w=0.4, n_jobs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strange behavior\n",
    "\n",
    "For whatever reason, the imports below result in the above initizalizaiton of mg not finishing, so we've loaded mg first, then will continue the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse, csc_matrix, csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformer\n",
    "\n",
    "We use an inverse hyperbolic sine transformation to transform the features.\n",
    "\n",
    "The transormation is thus:\n",
    "\n",
    "$$sinh^{-1} \\left( \\frac{X_{mp}}{\\sigma_{mp}} \\right)$$\n",
    "\n",
    "where $X_{mp}$ is the column in the dwpc feature matrix $X$ coreesponding to metapath $mp$ and $\\sigma_{mp}$ is the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanScaledArcsinhTransformer(TransformerMixin):\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            self.initial_mean_ = X.tocoo().tocsc().mean(axis=0).A[0]\n",
    "        else:\n",
    "            self.initial_mean_ = X.mean(axis=0)\n",
    "\n",
    "        # If input was DataFrame, Converts resultant series to ndarray\n",
    "        try:\n",
    "            self.initial_mean_ = self.initial_mean_.values\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If inital mean == 0, likely all values were zero\n",
    "        # this prevents issues later.\n",
    "        self.initial_mean_[np.where(self.initial_mean_ == 0.0)] = 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return np.arcsinh(X.tocoo().tocsc().multiply(self.initial_mean_**-1))\n",
    "        return np.arcsinh(X / self.initial_mean_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homegrown Feature Selector\n",
    "\n",
    "Runs 6 analysis on the traning data to select features.\n",
    "\n",
    "1. Correlation to the output\n",
    "2. Chi_squared test\n",
    "3. Recurssive Feature Elimantion on a Ridge Regressor\n",
    "4. Embedded Feature Selection from a Lasso Regressor\n",
    "5. Embedded Feature Selection from a Randomn Forest Classifier\n",
    "6. Embedded Feature Selection from a Gradient Boosting Classifier\n",
    "\n",
    "Each analysis will select `num_feats` best features. The selected features will then by chosen via a voting method with `min_selections` out of the 6 elements required to for a feature to be kept. \n",
    "\n",
    "\n",
    "We have also added an option for `always_keep`:  This allows for domain expertise to be factored into the feature selection process.  In our case, we know some metapaths are specifically mechanistic, so we want to include those wherever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y, feature_names, num_feats):\n",
    "    cor_list = []\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in range(X.shape[1]):\n",
    "        if issparse(X):\n",
    "            x = X[:, i].A.reshape(len(y))\n",
    "        else:\n",
    "            x = X[:, i]\n",
    "        cor = np.corrcoef(x, y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = np.array(feature_names)[np.argsort(np.abs(cor_list))[-num_feats:].tolist()].tolist()    \n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    return [True if i in cor_feature else False for i in feature_names]\n",
    "\n",
    "def chi2_selector(X, y, num_feats):\n",
    "    this_selector = SelectKBest(chi2, k=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "def rfe_selector(X, y, num_feats, random_state=None):\n",
    "    this_selector = RFE(estimator=LogisticRegression(C=.1, solver='liblinear', random_state=random_state), \n",
    "                        n_features_to_select=num_feats, step=.2, verbose=5)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "def embeded_lr_selector(X, y, num_feats, random_state=None):\n",
    "    this_selector = SelectFromModel(LogisticRegression(penalty=\"l1\", solver='liblinear', random_state=random_state), \n",
    "                                    max_features=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "\n",
    "    return this_selector.get_support()\n",
    "\n",
    "def embeded_rf_selector(X, y, num_feats, n_jobs, random_state=None):\n",
    "    rfc = RandomForestClassifier(n_estimators=100, max_depth=50, n_jobs=n_jobs, random_state=random_state)\n",
    "    this_selector = SelectFromModel(rfc, max_features=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "def embeded_xgb_selector(X, y, num_feats, n_jobs=1, random_state=None):\n",
    "    # XGBoost takes 0 as default random state\n",
    "    if random_state is None:\n",
    "        random_state = 0\n",
    "    # Paramaters optimized for speed, rather than accuracy (as we have 5 other estimators also providing votes)\n",
    "    xgbc = XGBClassifier(max_depth=5, n_estimators=200, learning_rate=.16, min_child_weight=1, colsample_bytree=.8,\n",
    "                         n_jobs=n_jobs, random_state=random_state)\n",
    "    this_selector = SelectFromModel(xgbc, max_features=num_feats)\n",
    "    this_selector.fit(X, y)\n",
    "    return this_selector.get_support()\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_features=100, min_selections=4, n_jobs=1, feature_names=None, always_keep=None,\n",
    "                 random_state=None):\n",
    "        self.num_features = num_features\n",
    "        self.min_selections = min_selections\n",
    "        self.n_jobs = n_jobs\n",
    "        self.feature_names = feature_names\n",
    "        self.always_keep = always_keep\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "                \n",
    "        X_norm = MaxAbsScaler().fit_transform(X)\n",
    "        if issparse(X):\n",
    "            if type(X) != csc_matrix:\n",
    "                X = X.tocsc()\n",
    "            X_norm = X_norm.tocsc()\n",
    "        \n",
    "        print('Running Cor')\n",
    "        cor_support = cor_selector(X, y, self.feature_names, self.num_features)\n",
    "        print('Running Chi2')\n",
    "        chi_support = chi2_selector(X_norm, y, self.num_features)\n",
    "        print('Running RFE')\n",
    "        rfe_support = rfe_selector(X_norm, y, self.num_features, self.random_state)\n",
    "        print('Running LR')\n",
    "        embeded_lr_support = embeded_lr_selector(X_norm, y, self.num_features, self.random_state)\n",
    "        print('Running RF')\n",
    "        embeded_rf_support = embeded_rf_selector(X, y, self.num_features, \n",
    "                                                 n_jobs=self.n_jobs, random_state=self.random_state)\n",
    "        print('Running XG')\n",
    "        embeded_xgb_support = embeded_xgb_selector(X, y, self.num_features, \n",
    "                                                   n_jobs=self.n_jobs, random_state=self.random_state)\n",
    "        \n",
    "        feature_selection_df = pd.DataFrame({'feature':self.feature_names, 'pearson':cor_support, 'chi_2':chi_support, \n",
    "                                             'rfe':rfe_support, 'logistics':embeded_lr_support,\n",
    "                                             'random_forest':embeded_rf_support, 'xgboost':embeded_xgb_support})  \n",
    "\n",
    "        feature_selection_df['total'] = np.sum(feature_selection_df, axis=1)\n",
    "        self.feature_selection_df_ = feature_selection_df\n",
    "        \n",
    "        keep_features = feature_selection_df.query('total >= {}'.format(self.min_selections))['feature'].tolist()\n",
    "        \n",
    "        # Keep the features that we always want (e.g. domain expertise)\n",
    "        if self.always_keep is not None:\n",
    "            keep_features.extend(self.always_keep)\n",
    "        \n",
    "        self.keep_features_ = [f for f in self.feature_names if f in keep_features]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        if issparse(X) and type(X) != csc_matrix:\n",
    "            X = X.tocsc()\n",
    "        return X[:, [i for i, f in enumerate(self.feature_names) if f in self.keep_features_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_std(data, axis=1):                                                                              \n",
    "    \"\"\"take the standard deviation of a sparse matrix\"\"\"                  \n",
    "\n",
    "    def get_vec_std(vec):                                                                                       \n",
    "        return vec.A.std(ddof=1)                                                                                \n",
    "\n",
    "    stds = []        \n",
    "    \n",
    "    # ensure the correct matrix type for easy row or column subsetting\n",
    "    if axis==1 and type(data) != csc_matrix:\n",
    "        data = data.tocoo().tocsc()\n",
    "    if axis==0 and type(data) != csr_matrix:\n",
    "        data = data.tocoo().tocsr()\n",
    "    \n",
    "    # Get the std for each vector along the given axis individually\n",
    "    for i in range(data.shape[axis]):                                                                              \n",
    "        if axis==1:\n",
    "            stds.append(get_vec_std(data.getcol(i)))       \n",
    "        elif axis==0:\n",
    "            stds.append(get_vec_std(data.getrow(i)))       \n",
    "        \n",
    "    return np.array(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_coefs(model, X, f_names):                                                                                 \n",
    "    \"\"\"Helper Function to quickly return the model coefs and correspoding fetaure names\"\"\"                              \n",
    "                                                                                                                        \n",
    "    # Ensure we have a numpy array for the features                                                                     \n",
    "    if type(X) == pd.DataFrame:                                                                                         \n",
    "        X = X.values\n",
    "        \n",
    "                                                                                                                        \n",
    "    # Grab the coeffiencts                                                                                              \n",
    "    coef = model.coef_                                                                                                  \n",
    "    # Some models return a double dimension array, others only a single                                                 \n",
    "    if len(coef) != len(f_names):                                                                                       \n",
    "        coef = coef[0]                                                                                                  \n",
    "                                                                                                                        \n",
    "    # insert the intercept                                                                                              \n",
    "    coef = np.insert(coef, 0, model.intercept_)                                                                         \n",
    "    names = np.insert(f_names, 0, 'intercept')                                                                          \n",
    "                                                                                                                        \n",
    "    # Calculate z-score scaled coefficients based on the features                                                       \n",
    "    if issparse(X):\n",
    "        if type(X) != csc_matrix:\n",
    "            X = X.tocoo().tocsc()\n",
    "        z_intercept = coef[0] + sum(coef[1:] * X.mean(axis=0).A[0])\n",
    "        z_coef = coef[1:] * sparse_std(X, axis=1)\n",
    "        z_coef = np.insert(z_coef, 0, z_intercept)\n",
    "    else:\n",
    "        z_intercept = coef[0] + sum(coef[1:] * X.mean(axis=0))                                                              \n",
    "        z_coef = coef[1:] * X.std(axis=0)                                                                                   \n",
    "        z_coef = np.insert(z_coef, 0, z_intercept)                                                                          \n",
    "                                                                                                                        \n",
    "    # Return                                                                                                            \n",
    "    return pd.DataFrame([names, coef, z_coef]).T.rename(columns={0:'feature', 1:'coef', 2:'zcoef'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_size = 32 # bits\n",
    "bits_per_gb = 8589934592\n",
    "\n",
    "def print_mem_info(n_comp, n_dis, n_mps):\n",
    "    print(\"{:,} Compounds * {:,} Diseases = {:,} C-D Pairs\".format(n_comp, n_dis,\n",
    "                                                                   n_comp * n_dis))\n",
    "    print(\"{:,} C-D Pairs * {:,} Metapaths = {:,} Matrix Values\".format(n_comp * n_dis,\n",
    "                                                                    n_mps, \n",
    "                                                                    n_comp * n_dis * n_mps))\n",
    "\n",
    "    print('{:1,.1f} GB of matrix values'.format(n_comp * n_dis * n_mps * float_size / (bits_per_gb)))\n",
    "    \n",
    "    print('{:1,.3f} GB per metapath'.format(n_comp * n_dis * float_size / (bits_per_gb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.15\n",
    "rs = 20200123\n",
    "\n",
    "\n",
    "treat_comps = set(edges.query('type == \"treats_CtD\"')['start_id'])\n",
    "# Sample the negatives and subsample \n",
    "keep_comps = set(nodes.query('id not in @treat_comps and label == \"ChemicalSubstance\"')\n",
    "                      .sample(frac=train_frac*.01, random_state=rs)['id'])\n",
    "# Then subsample the positives\n",
    "keep_comps = keep_comps | set(nodes.query('id in @treat_comps')\n",
    "                                   .sample(frac=train_frac, random_state=rs+1)['id'])\n",
    "\n",
    "treat_dis = set(edges.query('type == \"treats_CtD\"')['end_id'])\n",
    "# Sample the negatives and subsample cv\n",
    "keep_dis = set(nodes.query('label == \"Disease\" and id not in @treat_dis')\n",
    "                    .sample(frac=train_frac*.01, random_state=rs+2)['id'])\n",
    "# Take the diseases Treated by these compounds\n",
    "keep_dis = keep_dis | set(edges.query('type == \"treats_CtD\" and start_id in @keep_comps')['end_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mp</th>\n",
       "      <th>pair_count</th>\n",
       "      <th>subset</th>\n",
       "      <th>frac</th>\n",
       "      <th>sim_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CaAawD</td>\n",
       "      <td>19950</td>\n",
       "      <td>all_pairs</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CaAsoD</td>\n",
       "      <td>0</td>\n",
       "      <td>all_pairs</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mp  pair_count     subset     frac  sim_mp\n",
       "0  CaAawD       19950  all_pairs  0.00004   False\n",
       "1  CaAsoD           0  all_pairs  0.00000   False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mp_counts = pd.read_csv('../2_pipeline/11b_Model_Prep_Metapath_Membership_Analysis/out/all_mp_counts.csv')\n",
    "all_mp_counts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sim_names = all_mp_counts.query('sim_mp == False')['mp'].unique().tolist()\n",
    "mp_qr = all_mp_counts.query('subset == \"all_pairs\" and mp in @non_sim_names and pair_count > 0')\n",
    "good_mps = mp_qr['mp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,292 Compounds * 2,903 Diseases = 3,750,676 C-D Pairs\n",
      "3,750,676 C-D Pairs * 7,303 Metapaths = 27,391,186,828 Matrix Values\n",
      "102.0 GB of matrix values\n",
      "0.014 GB per metapath\n"
     ]
    }
   ],
   "source": [
    "print_mem_info(len(keep_comps), len(keep_dis), len(good_mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,305 Positive training examples in subset\n"
     ]
    }
   ],
   "source": [
    "print('{:,} Positive training examples in subset'.format(len(edges.query('start_id in @keep_comps and type == \"treats_CtD\"'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in the known metapaths found in DrugMechDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mech_mps = pd.read_csv('../0_data/manual/mech_mps.txt', header=None)[0].values\n",
    "dmdb_feat = set(all_mp_counts.query('mp in @mech_mps and subset == \"all_pairs\" and pair_count > 0 and sim_mp == False')['mp'])\n",
    "\n",
    "len(dmdb_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the features one time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pair counts to sort metapaths extaction as a niave load balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_mps_for_pw_extraction(n_big_calcs, big_per_block, mp_list, frac_info):\n",
    "    \n",
    "    big_mp = frac_info.sort_values('frac', ascending=False).head(n_big_calcs)['mp'].tolist()\n",
    "    other_mp = list(set(mp_list) - set(big_mp))\n",
    "\n",
    "    block_size = len(other_mp) // (len(big_mp) // big_per_block)\n",
    "    n_blocks = ((len(big_mp) + len(other_mp)) // block_size)\n",
    "\n",
    "    out = []\n",
    "    for i in range(n_blocks):\n",
    "        for j in range(big_per_block):\n",
    "            idx = i*big_per_block + j\n",
    "            out.append(big_mp[idx])\n",
    "        out += other_mp[i*block_size:(i+1)*block_size] \n",
    "        \n",
    "    out += list(set(other_mp) - set(out))    \n",
    "    \n",
    "    return out, block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30, 5 finishes < 30 min.\n",
    "# 40, 8 finishes 18min 46s\n",
    "to_xtract, block_size = sort_mps_for_pw_extraction(40, 8, good_mps, mp_qr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 1452)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_xtract), block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [19:57<00:00, 199.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 21s, sys: 1min 3s, total: 6min 25s\n",
      "Wall time: 19min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Extract the metapaths to do some prep work\n",
    "(pairs, feats), test_dwpc = piecewise_extraction(function=mg.extract_dwpc, \n",
    "                                 to_split='metapaths', block_size=block_size,\n",
    "                                 axis=1,\n",
    "                                 metapaths=to_xtract, \n",
    "                                 start_nodes=list(keep_comps), \n",
    "                                 end_nodes=list(keep_dis),\n",
    "                                 return_sparse=True,\n",
    "                                 sparse_df=False,\n",
    "                                 n_jobs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Targets for feature selection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHEBI:100147', 'DOID:0050400'), ('CHEBI:100147', 'DOID:13148')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tups = edges.query('type == \"treats_CtD\"')[['start_id', 'end_id']].apply(tuple, axis=1).tolist()\n",
    "pos_tups[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tups = set(pos_tups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f917e66392f440ac8fe0e34009df96e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3750676.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3750676 3750676 8305\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for row in tqdm(pairs.itertuples(), total=len(pairs)):\n",
    "    if set([(row.chemicalsubstance_id, row.disease_id)]) & pos_tups:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "\n",
    "y = np.array(y)\n",
    "        \n",
    "print(len(pairs), len(y), sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs['status'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample the potnetial training examples\n",
    "\n",
    "Our class is so imbalanced, to get a sizeable number of positive training examples, we end up with many orders of magnitude more negative examples.  Many of those examples will have no connections from the compound to the disease of interest, this providing a zero row in the matrix. We will not waste time training on those values, and instead focus on the ones that distinguish the positive from the negative training examples.\n",
    "\n",
    "As this is just hyperparameter tunings, to speed things up, we will also limit the positive examples to a small portion of the negative examples, 100x (2 orders of magnitude) larger than the number of postitives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows that are Non-zero\n",
    "nz_index = pairs[test_dwpc.getnnz(1)>0].index\n",
    "\n",
    "# have the number of postivies to get 100x this for the negatives.\n",
    "n_pos = pairs['status'].sum()\n",
    "\n",
    "# Sample the nonzero negative examples at a rate of 100x the positive samples\n",
    "neg_index = pairs.loc[nz_index].query('status == 0').sample(n=100*n_pos, random_state=rs+10).sort_index().index\n",
    "\n",
    "# and of course take the training postivies\n",
    "pos_index = pairs.query('status == 1').index\n",
    "\n",
    "# Union the two\n",
    "train_index = pos_index.union(neg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.array(feats)\n",
    "nz_feats = feats[test_dwpc.getnnz(0)>0]\n",
    "feat_index = test_dwpc.getnnz(0)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our compounds as ndarrays for easy indexing with sklearns StratifiedKFold\n",
    "keep_comps = np.array(list(keep_comps))\n",
    "# Need to know how to properly stratify the split\n",
    "is_treat_comp = np.array([1 if c in treat_comps else 0 for c in keep_comps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the features\n",
    "\n",
    "This is a time consuming and costly step. We will do once with the initial DWPC for parameter tuning. We will perform again at the end with the selected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 s, sys: 10.9 s, total: 53.9 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "msat = MeanScaledArcsinhTransformer()\n",
    "trans_dwpc = msat.fit_transform(test_dwpc[train_index][:, feat_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chi2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RFE\n",
      "Fitting estimator with 5607 features.\n",
      "Fitting estimator with 4486 features.\n",
      "Fitting estimator with 3365 features.\n",
      "Fitting estimator with 2244 features.\n",
      "Fitting estimator with 1123 features.\n",
      "Running LR\n",
      "Running RF\n",
      "Running XG\n",
      "CPU times: user 2h 47min 42s, sys: 3min 10s, total: 2h 50min 53s\n",
      "Wall time: 15min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fsel = FeatureSelector(num_features=500, min_selections=4, n_jobs=30, \n",
    "                       feature_names=nz_feats.tolist(), always_keep=dmdb_feat, random_state=rs+5)\n",
    "sel_dwpc = fsel.fit_transform(trans_dwpc, y[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.4 s, sys: 348 ms, total: 59.8 s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e_net = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=500, \n",
    "                           **{'l1_ratio': 0.10455936193818496, 'C': 0.2}, random_state=rs+6)\n",
    "e_net.fit(sel_dwpc, y[train_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_name = '11c_Model_Prep_Hyperparam_tuning'\n",
    "\n",
    "out_dir = Path('../2_pipeline').joinpath(this_name, 'out').resolve()\n",
    "tmp_dir = out_dir.parent.joinpath('tmp')\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dwpc = dict()\n",
    "\n",
    "def get_dwpc(w):\n",
    "    global all_dwpc\n",
    "    \n",
    "    # only extract if not previously extracted\n",
    "    if w not in all_dwpc.keys():\n",
    "        mg.update_w(w)\n",
    "        (pairs, feats), dwpc = piecewise_extraction(function=mg.extract_dwpc, \n",
    "                                 to_split='metapaths', block_size=block_size,\n",
    "                                 axis=1,\n",
    "                                 metapaths=to_xtract, \n",
    "                                 start_nodes=list(keep_comps), \n",
    "                                 end_nodes=list(keep_dis),\n",
    "                                 return_sparse=True,\n",
    "                                 sparse_df=False,\n",
    "                                 n_jobs=30)\n",
    "        # next step is split, so need sparse rows\n",
    "        dwpc = dwpc.tocoo().tocsr()\n",
    "        #all_dwpc[w] = dwpc\n",
    "        return dwpc\n",
    "    else:\n",
    "        return all_dwpc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "\n",
    "def hyperopt(param_space, y, feats, num_eval):\n",
    "\n",
    "    def objective_function(params):\n",
    "        dwpc_params = {k.split('__')[1]: v for k, v in params.items() if k.split('__')[0] == 'dwpc'}\n",
    "        enet_params = {k.split('__')[1]: v for k, v in params.items() if k.split('__')[0] == 'enet'}\n",
    "\n",
    "        this_w = dwpc_params['w']\n",
    "\n",
    "        # Set up the post feature extraction pipeline\n",
    "        post_extraction_pipeline = Pipeline(\n",
    "            [('transformer', MeanScaledArcsinhTransformer()),\n",
    "             ('maxabs_scale', MaxAbsScaler()),\n",
    "             ('e_net', LogisticRegression(penalty='elasticnet', solver='saga', max_iter=100, **enet_params,\n",
    "                                          random_state=rs+6))], verbose=True)\n",
    "\n",
    "        ## Get the dwpc information for the current pairs\n",
    "        dwpc = get_dwpc(this_w)\n",
    "        this_dwpc = dwpc[train_index][:, feat_index]\n",
    "        this_dwpc = fsel.transform(this_dwpc)\n",
    "\n",
    "        cv = cross_validate(post_extraction_pipeline, this_dwpc, y, cv=5, \n",
    "                            scoring=['average_precision', 'roc_auc'], return_estimator=True)\n",
    "\n",
    "        # Write out scores for each run\n",
    "        with open(tmp_dir.joinpath('scores_w_{0:1.4f}_C_{1:1.5f}_l1_{2:1.4f}.txt'.format(\n",
    "                                   this_w, enet_params['C'], enet_params['l1_ratio'])), 'w') as f_out:\n",
    "\n",
    "            f_out.write(', '.join([str(s) for s in cv['test_average_precision']]))\n",
    "            f_out.write('\\n')\n",
    "            f_out.write(', '.join([str(s) for s in cv['test_roc_auc']]))\n",
    "            f_out.write('\\n')\n",
    "\n",
    "        score = cv['test_average_precision'].mean()\n",
    "        print('Mean Score: {:1.4f}'.format(score))\n",
    "        \n",
    "        return {'loss': -1*score, 'status': STATUS_OK}\n",
    "\n",
    "    start = time()\n",
    "   \n",
    "    \n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective_function, \n",
    "                      param_space, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate= np.random.RandomState(1))\n",
    "    \n",
    "    print(time() - start)\n",
    "    return trials, best_param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous best before adding new keep_features: \n",
    "# '1l_ratio': 0.10455936193818496, 'C': 0.000556880900960339, 'w': 0.2640929485381926\n",
    "param_hyperopt = {\n",
    "    'dwpc__w': hp.uniform('w', 0.01, 1),\n",
    "    'enet__C': hp.loguniform('C', np.log(0.0001), np.log(.2)),\n",
    "    'enet__l1_ratio': hp.uniform('l1_ratio', .01, .99),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   Changing w from 0.7056185425199153 to 0.2471551211320533. Please wait...\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      "  2%|2         | 2/96 [00:00<00:11,  8.22it/s]\n",
      "\u001b[A\u001b[A\n",
      " 10%|#         | 10/96 [00:00<00:07, 11.25it/s]\n",
      "\u001b[A\u001b[A\n",
      " 18%|#7        | 17/96 [00:00<00:05, 14.88it/s]\n",
      "\u001b[A\u001b[A\n",
      " 27%|##7       | 26/96 [00:00<00:03, 19.46it/s]\n",
      "\u001b[A\u001b[A\n",
      " 35%|###5      | 34/96 [00:00<00:02, 25.06it/s]\n",
      "\u001b[A\u001b[A\n",
      " 56%|#####6    | 54/96 [00:00<00:01, 33.81it/s]\n",
      "\u001b[A\u001b[A\n",
      " 70%|######9   | 67/96 [00:00<00:00, 43.41it/s]\n",
      "\u001b[A\u001b[A\n",
      " 81%|########1 | 78/96 [00:01<00:00, 50.48it/s]\n",
      "\u001b[A\u001b[A\n",
      " 92%|#########1| 88/96 [00:01<00:00, 50.47it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:01<00:00, 50.47it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      " 17%|#6        | 1/6 [01:06<05:34, 66.95s/it]\n",
      "\u001b[A\u001b[A\n",
      " 33%|###3      | 2/6 [04:50<07:35, 113.89s/it]\n",
      "\u001b[A\u001b[A\n",
      " 50%|#####     | 3/6 [08:29<07:16, 145.45s/it]\n",
      "\u001b[A\u001b[A\n",
      " 67%|######6   | 4/6 [12:12<05:37, 168.85s/it]\n",
      "\u001b[A\u001b[A\n",
      " 83%|########3 | 5/6 [18:04<03:43, 223.65s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:21<00:00, 215.60s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:21<00:00, 213.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.8s\n",
      "\n",
      "  0%|          | 0/50 [22:16<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.2s\n",
      "\n",
      "  0%|          | 0/50 [22:19<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.6min\n",
      "\n",
      "  0%|          | 0/50 [23:57<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.7s\n",
      "\n",
      "  0%|          | 0/50 [24:02<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   1.9s\n",
      "\n",
      "  0%|          | 0/50 [24:03<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.6min\n",
      "\n",
      "  0%|          | 0/50 [25:41<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.0s\n",
      "\n",
      "  0%|          | 0/50 [25:46<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.4s\n",
      "\n",
      "  0%|          | 0/50 [25:49<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.7min\n",
      "\n",
      "  0%|          | 0/50 [27:30<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.3s\n",
      "\n",
      "  0%|          | 0/50 [27:35<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.8s\n",
      "\n",
      "  0%|          | 0/50 [27:37<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.7min\n",
      "\n",
      "  0%|          | 0/50 [29:22<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.0s\n",
      "\n",
      "  0%|          | 0/50 [29:26<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.3s\n",
      "\n",
      "  0%|          | 0/50 [29:28<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                   [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.7min\n",
      "\n",
      "  0%|          | 0/50 [31:09<?, ?trial/s, best loss=?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                   Mean Score: 0.0831\n",
      "\n",
      "  0%|          | 0/50 [31:10<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏         | 1/50 [31:10<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    Changing w from 0.2471551211320533 to 0.7056185425199153. Please wait...\n",
      "\n",
      "  2%|▏         | 1/50 [31:10<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      "  2%|2         | 2/96 [00:00<00:12,  7.75it/s]\n",
      "\u001b[A\u001b[A\n",
      "  5%|5         | 5/96 [00:00<00:09,  9.45it/s]\n",
      "\u001b[A\u001b[A\n",
      " 19%|#8        | 18/96 [00:00<00:06, 12.94it/s]\n",
      "\u001b[A\u001b[A\n",
      " 27%|##7       | 26/96 [00:00<00:04, 17.24it/s]\n",
      "\u001b[A\u001b[A\n",
      " 36%|###6      | 35/96 [00:00<00:02, 22.31it/s]\n",
      "\u001b[A\u001b[A\n",
      " 46%|####5     | 44/96 [00:00<00:01, 28.68it/s]\n",
      "\u001b[A\u001b[A\n",
      " 60%|######    | 58/96 [00:00<00:01, 37.62it/s]\n",
      "\u001b[A\u001b[A\n",
      " 73%|#######2  | 70/96 [00:01<00:00, 45.93it/s]\n",
      "\u001b[A\u001b[A\n",
      " 82%|########2 | 79/96 [00:01<00:00, 53.39it/s]\n",
      "\u001b[A\u001b[A\n",
      " 92%|#########1| 88/96 [00:01<00:00, 60.61it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:02<00:00, 47.40it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      " 17%|#6        | 1/6 [01:07<05:38, 67.61s/it]\n",
      "\u001b[A\u001b[A\n",
      " 33%|###3      | 2/6 [04:46<07:31, 112.88s/it]\n",
      "\u001b[A\u001b[A\n",
      " 50%|#####     | 3/6 [08:28<07:17, 145.71s/it]\n",
      "\u001b[A\u001b[A\n",
      " 67%|######6   | 4/6 [12:09<05:36, 168.30s/it]\n",
      "\u001b[A\u001b[A\n",
      " 83%|########3 | 5/6 [18:02<03:43, 223.78s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:20<00:00, 216.04s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:20<00:00, 213.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.8s\n",
      "\n",
      "  2%|▏         | 1/50 [53:29<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.1s\n",
      "\n",
      "  2%|▏         | 1/50 [53:31<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.7min\n",
      "\n",
      "  2%|▏         | 1/50 [55:12<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.7s\n",
      "\n",
      "  2%|▏         | 1/50 [55:17<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.0s\n",
      "\n",
      "  2%|▏         | 1/50 [55:19<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ............. (step 3 of 3) Processing e_net, total=  24.9s\n",
      "\n",
      "  2%|▏         | 1/50 [55:43<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.0s\n",
      "\n",
      "  2%|▏         | 1/50 [55:49<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.4s\n",
      "\n",
      "  2%|▏         | 1/50 [55:51<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ............. (step 3 of 3) Processing e_net, total= 1.4min\n",
      "\n",
      "  2%|▏         | 1/50 [57:18<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.3s\n",
      "\n",
      "  2%|▏         | 1/50 [57:23<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.6s\n",
      "\n",
      "  2%|▏         | 1/50 [57:25<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ............. (step 3 of 3) Processing e_net, total=  18.4s\n",
      "\n",
      "  2%|▏         | 1/50 [57:44<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.0s\n",
      "\n",
      "  2%|▏         | 1/50 [57:47<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.3s\n",
      "\n",
      "  2%|▏         | 1/50 [57:50<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    [Pipeline] ............. (step 3 of 3) Processing e_net, total=  21.3s\n",
      "\n",
      "  2%|▏         | 1/50 [58:11<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    Mean Score: 0.0773\n",
      "\n",
      "  2%|▏         | 1/50 [58:12<25:27:46, 1870.75s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "  4%|▍         | 2/50 [58:13<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                    Changing w from 0.7056185425199153 to 0.6867218654755485. Please wait...\n",
      "\n",
      "  4%|▍         | 2/50 [58:13<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      "  1%|1         | 1/96 [00:00<00:17,  5.38it/s]\n",
      "\u001b[A\u001b[A\n",
      "  2%|2         | 2/96 [00:00<00:18,  4.99it/s]\n",
      "\u001b[A\u001b[A\n",
      " 11%|#1        | 11/96 [00:00<00:12,  6.96it/s]\n",
      "\u001b[A\u001b[A\n",
      " 19%|#8        | 18/96 [00:00<00:08,  9.32it/s]\n",
      "\u001b[A\u001b[A\n",
      " 26%|##6       | 25/96 [00:00<00:05, 12.42it/s]\n",
      "\u001b[A\u001b[A\n",
      " 35%|###5      | 34/96 [00:00<00:03, 16.49it/s]\n",
      "\u001b[A\u001b[A\n",
      " 48%|####7     | 46/96 [00:01<00:02, 22.23it/s]\n",
      "\u001b[A\u001b[A\n",
      " 64%|######3   | 61/96 [00:01<00:01, 29.73it/s]\n",
      "\u001b[A\u001b[A\n",
      " 76%|#######6  | 73/96 [00:01<00:00, 38.11it/s]\n",
      "\u001b[A\u001b[A\n",
      " 90%|########9 | 86/96 [00:01<00:00, 44.80it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:02<00:00, 25.27it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:02<00:00, 43.07it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      " 17%|#6        | 1/6 [01:10<05:50, 70.19s/it]\n",
      "\u001b[A\u001b[A\n",
      " 33%|###3      | 2/6 [04:49<07:39, 114.93s/it]\n",
      "\u001b[A\u001b[A\n",
      " 50%|#####     | 3/6 [08:32<07:22, 147.36s/it]\n",
      "\u001b[A\u001b[A\n",
      " 67%|######6   | 4/6 [12:16<05:40, 170.32s/it]\n",
      "\u001b[A\u001b[A\n",
      " 83%|########3 | 5/6 [18:10<03:45, 225.41s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:29<00:00, 217.39s/it]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 6/6 [21:29<00:00, 214.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                                                    [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.8s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:41<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.1s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:43<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ............. (step 3 of 3) Processing e_net, total=   3.1s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:46<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.7s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:51<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.0s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:53<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ............. (step 3 of 3) Processing e_net, total=   3.0s\n",
      "\n",
      "  4%|▍         | 2/50 [1:20:56<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.0s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:01<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.4s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:03<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ............. (step 3 of 3) Processing e_net, total=   1.0s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:04<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ....... (step 1 of 3) Processing transformer, total=   3.3s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:09<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.6s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:12<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ............. (step 3 of 3) Processing e_net, total=   0.7s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:13<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ....... (step 1 of 3) Processing transformer, total=   2.9s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:16<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ...... (step 2 of 3) Processing maxabs_scale, total=   2.3s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:19<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      [Pipeline] ............. (step 3 of 3) Processing e_net, total=   0.7s\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:19<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      Mean Score: 0.0099\n",
      "\n",
      "  4%|▍         | 2/50 [1:21:21<23:56:58, 1796.22s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "  6%|▌         | 3/50 [1:21:21<21:51:09, 1673.83s/trial, best loss: -0.08311038010471776]\u001b[A\n",
      "\u001b[A                                                                                      Changing w from 0.6867218654755485 to 0.8320950220852148. Please wait...\n",
      "\n",
      "  6%|▌         | 3/50 [1:21:21<21:51:09, 1673.83s/trial, best loss: -0.08311038010471776]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      "  1%|1         | 1/96 [00:00<00:17,  5.33it/s]\n",
      "\u001b[A\u001b[A\n",
      "  2%|2         | 2/96 [00:00<00:19,  4.92it/s]\n",
      "\u001b[A\u001b[A\n",
      " 15%|#4        | 14/96 [00:00<00:11,  6.88it/s]\n",
      "\u001b[A\u001b[A\n",
      " 26%|##6       | 25/96 [00:00<00:07,  9.55it/s]\n",
      "\u001b[A\u001b[A\n",
      " 43%|####2     | 41/96 [00:00<00:04, 13.26it/s]\n",
      "\u001b[A\u001b[A\n",
      " 54%|#####4    | 52/96 [00:00<00:02, 18.01it/s]\n",
      "\u001b[A\u001b[A\n",
      " 64%|######3   | 61/96 [00:00<00:01, 23.43it/s]\n",
      "\u001b[A\u001b[A\n",
      " 78%|#######8  | 75/96 [00:01<00:00, 31.03it/s]\n",
      "\u001b[A\u001b[A\n",
      " 91%|######### | 87/96 [00:01<00:00, 37.15it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:02<00:00, 20.50it/s]\n",
      "\u001b[A\u001b[A\n",
      "100%|##########| 96/96 [00:02<00:00, 44.19it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\u001b[A\u001b[A\n",
      " 17%|#6        | 1/6 [01:11<05:58, 71.64s/it]\n",
      "\u001b[A\u001b[A\n",
      " 33%|###3      | 2/6 [04:57<07:51, 117.76s/it]\n",
      "\u001b[A\u001b[A\n",
      " 50%|#####     | 3/6 [08:44<07:32, 150.76s/it]\n",
      "\u001b[A\u001b[A\n",
      " 67%|######6   | 4/6 [12:32<05:47, 173.82s/it]\n",
      "\u001b[A\u001b[A\n",
      " 83%|########3 | 5/6 [18:25<03:47, 227.49s/it]\n",
      "\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "trials, best_param = hyperopt(param_hyperopt, y[train_index], to_xtract, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_param, open(out_dir.joinpath('best_param.pkl'), 'wb'))\n",
    "pickle.dump(trials, open(out_dir.joinpath('trials.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.06781152637797157,\n",
       " 'l1_ratio': 0.2420458478281276,\n",
       " 'w': 0.5267957740707717}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the selected params to do the feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_params = {k: v for k, v in best_param.items() if k != 'w'}\n",
    "\n",
    "post_extraction_pipeline = Pipeline(\n",
    "    [('transformer', MeanScaledArcsinhTransformer()),\n",
    "     ('feature_selection', FeatureSelector(num_features=500, min_selections=4, n_jobs=30,\n",
    "                                           feature_names=to_xtract, always_keep=dmdb_feat,\n",
    "                                           random_state=4))], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing w from 0.7857592349063235 to 0.5267957740707717. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/96 [00:00<00:21,  4.51it/s]\u001b[A\n",
      "  2%|▏         | 2/96 [00:00<00:20,  4.59it/s]\u001b[A\n",
      " 10%|█         | 10/96 [00:00<00:13,  6.39it/s]\u001b[A\n",
      " 16%|█▌        | 15/96 [00:00<00:09,  8.64it/s]\u001b[A\n",
      " 21%|██        | 20/96 [00:00<00:06, 11.25it/s]\u001b[A\n",
      " 34%|███▍      | 33/96 [00:00<00:04, 15.48it/s]\u001b[A\n",
      " 42%|████▏     | 40/96 [00:01<00:02, 18.98it/s]\u001b[A\n",
      " 59%|█████▉    | 57/96 [00:01<00:01, 25.67it/s]\u001b[A\n",
      " 69%|██████▉   | 66/96 [00:01<00:01, 28.74it/s]\u001b[A\n",
      "100%|██████████| 96/96 [00:02<00:00, 46.69it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [01:41<08:25, 101.18s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [06:04<09:59, 149.86s/it]\u001b[A\n",
      " 50%|█████     | 3/6 [10:19<09:03, 181.33s/it]\u001b[A\n",
      " 67%|██████▋   | 4/6 [14:44<06:53, 206.56s/it]\u001b[A\n",
      " 83%|████████▎ | 5/6 [20:50<04:14, 254.39s/it]\u001b[A\n",
      "100%|██████████| 6/6 [24:48<00:00, 248.15s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "dwpc = get_dwpc(best_param['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 1 of 2) Processing transformer, total=  22.3s\n",
      "Running Cor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chi2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mmayers/software/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RFE\n",
      "Fitting estimator with 7303 features.\n",
      "Fitting estimator with 5843 features.\n",
      "Fitting estimator with 4383 features.\n",
      "Fitting estimator with 2923 features.\n",
      "Fitting estimator with 1463 features.\n",
      "Running LR\n",
      "Running RF\n",
      "Running XG\n",
      "[Pipeline] . (step 2 of 2) Processing feature_selection, total=14.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transformer',\n",
       "                 <__main__.MeanScaledArcsinhTransformer object at 0x7f7c86e63ba8>),\n",
       "                ('feature_selection',\n",
       "                 FeatureSelector(always_keep={'CaAawD', 'CaBPawD', 'CaGeBPawD',\n",
       "                                              'CaGiBPawD', 'CaGnrBPawD',\n",
       "                                              'CaGpoAawD', 'CaGpoGeBPawD',\n",
       "                                              'CaGpoGnrBPawD', 'CaGpoGprBPawD',\n",
       "                                              'CaGpoPWiBPawD', 'CaGprBPawD',\n",
       "                                              'CaGprBPpAsoD', 'CaGrBPawD',\n",
       "                                              'CawPWawD', 'CiBPawD',\n",
       "                                              'C...\n",
       "                                                'ChoRXf>RXhiCdgD',\n",
       "                                                'CinGeBPprGmD', 'ChiRXiBPrGawD',\n",
       "                                                'CinGiBPrGtD', 'CaGnrBPnrGmD',\n",
       "                                                'CafGrRXhiCdgD',\n",
       "                                                'CinGhiRXnrGawD', 'CiBPiRXrGmD',\n",
       "                                                'CafGhoRXprBPawD',\n",
       "                                                'CinGhiRXhoCpvD',\n",
       "                                                'CinBPiFnrBPawD',\n",
       "                                                'ChiRXrBPprGawD',\n",
       "                                                'CinGhoRXpoCmD',\n",
       "                                                'CaBPprFpoAawD',\n",
       "                                                'CinBPprRXhiCtD', 'CinGrBPrGmD',\n",
       "                                                'CnrRXhoChiRXdD', ...],\n",
       "                                 min_selections=4, n_jobs=30, num_features=500,\n",
       "                                 random_state=4))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_extraction_pipeline.fit(dwpc[train_index], y[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_feats = post_extraction_pipeline[1].keep_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 196, 189)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pe_feats), len(fsel.keep_features_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(fsel.keep_features_) - set(pe_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = post_extraction_pipeline[1].feature_selection_df_\n",
    "feat_df.sort_values('total', ascending=False).to_csv(out_dir.joinpath('feature_selection_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(post_extraction_pipeline[1], open(out_dir.joinpath('feature_selector.pkl'), 'wb'))\n",
    "pd.Series(post_extraction_pipeline[1].keep_features_).to_csv(out_dir.joinpath('kept_features.txt'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
